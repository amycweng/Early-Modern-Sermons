{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "'''\n",
    "Obtain all words that have a part of speech of 'ab' (abbreviation) or 'nn1' (singular proper noun) in EP XML sermons\n",
    "'''\n",
    "import os,json,sys \n",
    "sys.path.append('../')\n",
    "from lib.standardization import * \n",
    "# dictionary that maps lemmas to variants \n",
    "lemma_dict = {l:{a:0 for a in a_list} for l, a_list in abbrev.items()}\n",
    "\n",
    "def lemmas_nouns_and_abbrev(filepath):\n",
    "    # read the input XML file \n",
    "    with open(filepath,'r',encoding='utf-8') as file: \n",
    "        data = file.read()\n",
    "\n",
    "    # use soupstrainer to only parse the main text of the book \n",
    "    # create a parsed tree using the xml parser for fast performance \n",
    "    soup = BeautifulSoup(data,features=\"lxml-xml\",parse_only=SoupStrainer(\"w\"))\n",
    "    \n",
    "    # iterate through every word\n",
    "    words = soup.find_all('w')\n",
    "    for w in words: \n",
    "        pos = w.get(\"pos\")\n",
    "        if pos == 'ab' or pos == 'nn1':\n",
    "            l = clean_word(w.get(\"lemma\"))\n",
    "            if l not in lemma_dict and l not in abbrev_to_book: \n",
    "                # entirely new word that is not in either the key or values of the dictionary of book abbreviations \n",
    "                lemma_dict[l] = {}\n",
    "            elif l in abbrev_to_book: \n",
    "                # lemma is already found in the dictionary of book abbreviations \n",
    "                # set the key to the standardized form \n",
    "                l = abbrev_to_book[l]\n",
    "            word = clean_word(w.text)\n",
    "            if word not in lemma_dict[l]: \n",
    "                # this variant has not been seen before\n",
    "                lemma_dict[l][word] = 0 \n",
    "            lemma_dict[l][word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 texts\n",
      "Processed 200 texts\n",
      "Processed 300 texts\n",
      "Processed 400 texts\n",
      "Processed 500 texts\n",
      "Processed 600 texts\n",
      "Processed 700 texts\n",
      "Processed 800 texts\n",
      "Processed 900 texts\n",
      "Processed 1000 texts\n",
      "Processed 1100 texts\n",
      "Processed 1200 texts\n",
      "Processed 1300 texts\n",
      "Processed 1400 texts\n",
      "Processed 1500 texts\n",
      "Processed 1600 texts\n",
      "Processed 1700 texts\n",
      "Processed 1800 texts\n",
      "Processed 1900 texts\n",
      "Processed 2000 texts\n",
      "Processed 2100 texts\n",
      "Processed 2200 texts\n",
      "Processed 2300 texts\n",
      "Processed 2400 texts\n",
      "Processed 2500 texts\n",
      "Processed 2600 texts\n",
      "Processed 2700 texts\n",
      "Processed 2800 texts\n",
      "Processed 2900 texts\n",
      "Processed 3000 texts\n",
      "Processed 3100 texts\n",
      "Processed 3200 texts\n",
      "Processed 3300 texts\n",
      "Processed 3400 texts\n",
      "Processed 3500 texts\n",
      "Processed 3600 texts\n",
      "Processed 3700 texts\n",
      "Processed 3800 texts\n",
      "Processed 3900 texts\n",
      "Processed 4000 texts\n",
      "Processed 4100 texts\n",
      "Processed 4200 texts\n",
      "Processed 4300 texts\n",
      "Processed 4400 texts\n",
      "Processed 4500 texts\n",
      "Processed 4600 texts\n",
      "Processed 4700 texts\n",
      "Processed 4800 texts\n",
      "Processed 4900 texts\n",
      "Processed 5000 texts\n",
      "Processed 5100 texts\n"
     ]
    }
   ],
   "source": [
    "import os,json\n",
    "''' \n",
    "Took 164 minutes \n",
    "'''\n",
    "ep = \"/Users/amycweng/Digital Humanities/sermonsEP\"\n",
    "with open(\"../assets/sermons_ep.json\",\"r\") as file: \n",
    "    sermons_ep = json.load(file)\n",
    "\n",
    "count = 0\n",
    "for file in sermons_ep: \n",
    "    filepath = os.path.join(ep,file)\n",
    "    lemmas_nouns_and_abbrev(filepath)\n",
    "    count += 1 \n",
    "    if count % 100 == 0: \n",
    "        print(f\"Processed {count} texts\")\n",
    "    if count % 500 == 0: # save progress in case anything goes wrong\n",
    "        with open(f\"../assets/lemmas_{count}.json\",\"w+\") as file: \n",
    "            json.dump(lemma_dict,file)\n",
    "with open(f\"../assets/lemmas.json\",\"w+\") as file: \n",
    "    json.dump(lemma_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../assets/lemmas_sorted.json\",\"w+\") as file: \n",
    "    json.dump(sorted(lemma_dict.items()),file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "with open(f\"../assets/lemmas.json\",\"r\") as file: \n",
    "    lemma_dict = json.load(file)\n",
    "\n",
    "possible_matches = {k:[] for k in abbrev}\n",
    "\n",
    "for lemma in sorted(lemma_dict):\n",
    "    if lemma not in abbrev: \n",
    "        most_likely = {}\n",
    "        # finding possible matches to any of the known abbreviations for each book \n",
    "        for ab, book in abbrev_to_book.items():\n",
    "            similarity = SequenceMatcher(None, ab, lemma).ratio()\n",
    "            if similarity >= 0.8:\n",
    "                possible_matches[book].append((similarity,ab,lemma_dict[lemma]))\n",
    "                most_likely[book] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_to_m = {k:{} for k in abbrev}\n",
    "for book, matches in possible_matches.items(): \n",
    "    for m in matches:\n",
    "        variants = m[2]\n",
    "        for v,freq in variants.items(): \n",
    "            # I forgot to strip colons and other punctuation marks earlier \n",
    "            # recompute frequencies here \n",
    "            v = re.sub(r\"[^a-z*]\",\"\",v)\n",
    "            if v not in b_to_m[book]:\n",
    "                b_to_m[book][v] = 0 \n",
    "            b_to_m[book][v] += freq \n",
    "\n",
    "with open(f\"../assets/variant_dict.json\",\"w+\") as file: \n",
    "    json.dump(b_to_m,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re\n",
    "\n",
    "with open('../assets/sermons_marginalia.csv', 'r') as file:          \n",
    "    notes = csv.reader(file, delimiter=',')\n",
    "    # find instances of possible numbered books\n",
    "    known, unknown = {}, {}  \n",
    "    for idx, entry in enumerate(notes):\n",
    "        # get note text \n",
    "        n = entry[-1]\n",
    "        # replace all periods with spaces and convert to lower case \n",
    "        n = re.sub(r'(\\.)',r' ',n).lower()\n",
    "        # replace all instances of two or more spaces with a single space. \n",
    "        n = re.sub(r'\\s+',' ',n)\n",
    "        possible = re.findall(r'\\b([a-z•]+)\\b \\b[0-9•]+\\b \\b[0-9•v]+\\b', n)\n",
    "        for i, p in enumerate(possible):\n",
    "            orig_p = p.split(\" \")[0]\n",
    "            p = clean_word(orig_p)\n",
    "            if p not in abbrev_to_book and p not in abbrev: \n",
    "                if p not in unknown: \n",
    "                    unknown[p] = 0 \n",
    "                unknown[p] += 1 \n",
    "            else: \n",
    "                if p in abbrev_to_book: \n",
    "                    book = abbrev_to_book[p]\n",
    "                if book not in known:\n",
    "                    known[book] = {} \n",
    "                if orig_p not in known[book]: \n",
    "                    known[book][orig_p] = 0\n",
    "                known[book][orig_p] += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = {k:{} for k in abbrev}\n",
    "for word in sorted(unknown): \n",
    "    for book, variants in b_to_m.items(): \n",
    "        if word == book or word in variants: \n",
    "            matches[book][word] = unknown[word]\n",
    "            break\n",
    "\n",
    "clean_format = []\n",
    "for book, variants in matches.items(): \n",
    "    for v,freq in variants.items():\n",
    "        clean_format.append(f\"{book}: {v} {freq}\\n\") \n",
    "    clean_format.append(\"\\n\")\n",
    "with open(f\"../assets/possible_variants.txt\",\"w+\") as file: \n",
    "    file.writelines(clean_format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
