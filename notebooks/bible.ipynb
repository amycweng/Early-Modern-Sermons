{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,re\n",
    "sys.path.append('../')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../assets/bible/kjv-adorned.txt','r') as file:\n",
    "    kjv_tokens = file.readlines()\n",
    "\n",
    "bible = {}\n",
    "current_ver = None\n",
    "for t in kjv_tokens:\n",
    "    t = t.split(\"\\t\")\n",
    "    token, pos, lemma = t[0], t[2], t[4]\n",
    "    if token[0].isupper() and re.search(r\"vv|n|uh\",pos):\n",
    "        lemma = token\n",
    "    if re.search(r'VERSE-',token):\n",
    "        current_ver = re.sub(\"VERSE-\", \"\",token)\n",
    "        if current_ver[0].islower():\n",
    "            current_ver = \"J\" + current_ver\n",
    "        # elif \"Acts\" in current_ver:\n",
    "        #     n = current_ver.split(\"-\")[-2:]\n",
    "        #     current_ver = f\"Acts-{n[0]}-{n[1]}\"\n",
    "        bible[current_ver] = []\n",
    "    else:\n",
    "        bible[current_ver].append(lemma)\n",
    "bible = {k: \" \".join(v) for k,v in bible.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "outputfolder = \"/Users/amycweng/DH/SERMONS_APP/db/data\"\n",
    "new = []\n",
    "# doc_id,version,part,book,chapter,verse,text\n",
    "kjv = pd.read_csv('../assets/bible/kjv.csv')\n",
    "for idx,verse_id in enumerate(kjv['doc_id']): \n",
    "    orig_id = verse_id\n",
    "    verse_id = verse_id.split(\"(KJV)\")[0].strip(\" \")\n",
    "    verse_id = re.sub(r\"[\\s\\:]\",\"-\",verse_id)\n",
    "    lemmatized = bible[verse_id]\n",
    "    new.append({0:orig_id, \n",
    "                1:kjv['version'][idx],\n",
    "                2:kjv['part'][idx],\n",
    "                3:kjv['book'][idx],\n",
    "                4:kjv['chapter'][idx],\n",
    "                5:kjv['verse'][idx],\n",
    "                6:kjv['text'][idx],\n",
    "                7:lemmatized})\n",
    "with open(f\"{outputfolder}/kjv.csv\",\"w+\") as outfile: \n",
    "    writer = csv.DictWriter(outfile, fieldnames=new[0].keys())\n",
    "    writer.writerows(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_verse(segment):\n",
    "    subsegments = re.split(r\"\\.|\\;|\\:|\\?\",segment)\n",
    "    to_segment = [\"but\", \", while\", \", let\", \", they\", \", NONLATINALPHABET\",\n",
    "                    \", then\", \", yet\", \", than\", ', and yet', ', and though',\n",
    "                    ', at least', ', and to', ', this be', ', for', ', therefore',\n",
    "                    ', that', ', and we', ', and i ', ', when', ', and say', ', and this',\n",
    "                    ', and then', ', and than', ', and they', ', i say', ', as the apostle',\n",
    "                    ', otherwise', ', how', ', according', ', accordi^^', ', say',', and when',\n",
    "                    ', and he', ', and she', ', he say', ', she say', ', lest', ', and where',\n",
    "                    ', and how', ', and what', ', and there', ', and therefore', ', and thus',\n",
    "                    ', and if', ', and because', ', and I ', ', he will', ', they will', ', she will']\n",
    "    pattern = '|'.join(map(re.escape, to_segment))\n",
    "    all_parts = []\n",
    "    for segment in subsegments: \n",
    "        parts = re.split(pattern, segment)\n",
    "        matches = re.findall(pattern,segment)\n",
    "        for idx, part in enumerate(parts):\n",
    "            if idx == (len(parts) - 1): break\n",
    "            # if len(part) == 0: continue\n",
    "            conj = re.sub(\", \", \"\",matches[idx])\n",
    "            parts[idx] = part\n",
    "            parts[idx + 1] = conj + parts[idx+1]\n",
    "        all_parts.extend(parts)\n",
    "    return_parts = [] \n",
    "    for part in all_parts:\n",
    "        part = part.strip(\" \")\n",
    "        if len(part) > 0 and not re.search(r\"^\\)\\s*$|^\\(\\s*$|^[bB]ehold[\\s\\,]*$|^say[\\s\\,]*|^and say[\\s\\,]*\",part): return_parts.append(part) \n",
    "    return return_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_bible = {}\n",
    "for label, verse in bible.items(): \n",
    "    segmented_bible[label] = split_verse(verse)\n",
    "bible_labels = []\n",
    "bible_parts = [] \n",
    "for label, verse in segmented_bible.items(): \n",
    "    for part in verse: \n",
    "        bible_parts.append(part)\n",
    "        bible_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/bible/bible_segments.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(norm=None, analyzer='word',sublinear_tf=True)\n",
    "tfidf_bible = vector.fit_transform(bible_parts)\n",
    "df = pd.DataFrame(tfidf_bible.toarray(), columns = vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores = df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average TFIDF score for each verse \n",
    "for idx, score in enumerate(doc_scores): \n",
    "    num_words = len(bible_parts[idx].split(\" \"))\n",
    "    doc_scores[idx] = score/num_words\n",
    "sorted_docs = np.argsort(doc_scores)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/bible_parts.txt','w') as file: \n",
    "    for idx in sorted_docs: \n",
    "        file.write(bible_labels[idx]+\"\\n\")\n",
    "        file.write(bible_parts[idx]+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
