{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,re\n",
    "sys.path.append('../')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "outputfolder = \"/Users/amycweng/DH/SERMONS_APP/db/data\"\n",
    "new = []\n",
    "bible = {}\n",
    "# doc_id,version,part,book,chapter,verse,text\n",
    "kjv = pd.read_csv('../assets/bible/kjv.csv')\n",
    "for idx,verse_id in enumerate(kjv['doc_id']): \n",
    "    orig_id = verse_id\n",
    "    verse_id = verse_id.split(\"(KJV)\")[0].strip(\" \")\n",
    "    verse_id = re.sub(r\"[\\s\\:]\",\"-\",verse_id)\n",
    "    bible[verse_id] = kjv['text'][idx]\n",
    "    new.append({0:orig_id, \n",
    "                1:kjv['version'][idx],\n",
    "                2:kjv['part'][idx],\n",
    "                3:kjv['book'][idx],\n",
    "                4:kjv['chapter'][idx],\n",
    "                5:kjv['verse'][idx],\n",
    "                6:kjv['text'][idx]})\n",
    "with open(f\"{outputfolder}/kjv.csv\",\"w+\") as outfile: \n",
    "    writer = csv.DictWriter(outfile, fieldnames=new[0].keys())\n",
    "    writer.writerows(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_verse(segment):\n",
    "    pattern = r'\\s+([,.?!;:)])'\n",
    "    segment = re.sub(pattern, r'\\1', segment)\n",
    "    pattern = r'([(])\\s+'\n",
    "    segment = re.sub(pattern, r'\\1', segment)\n",
    "    \n",
    "    if len(segment.split(\" \")) < 32: # below 75th percentile  \n",
    "        return [segment]\n",
    "    subsegments = re.split(r\"\\.|\\;|\\?|\\!\",segment)\n",
    "    # to_segment = [\"but\", \", while\", \", let\", \", they\", \", NONLATINALPHABET\",\n",
    "    #                 \", then\", \", yet\", \", than\", ', and yet', ', and though',\n",
    "    #                 ', at least', ', and to', ', this be', ', for', ', therefore',\n",
    "    #                 ', that', ', and we', ', and i ', ', when', ', and say', ', and this',\n",
    "    #                 ', and then', ', and than', ', and they', ', i say', ', as the apostle',\n",
    "    #                 ', otherwise', ', how', ', according', ', accordi^^', ', say',', and when',\n",
    "    #                 ', and he', ', and she', ', he say', ', she say', ', lest', ', and where',\n",
    "    #                 ', and how', ', and what', ', and there', ', and therefore', ', and thus',\n",
    "    #                 ', and if', ', and because', ', and I ', ', he will', ', they will', ', she will']\n",
    "    # pattern = '|'.join(map(re.escape, to_segment))\n",
    "    # all_parts = []\n",
    "    # for segment in subsegments: \n",
    "    #     parts = re.split(pattern, segment)\n",
    "    #     matches = re.findall(pattern,segment)\n",
    "    #     for idx, part in enumerate(parts):\n",
    "    #         if idx == (len(parts) - 1): break\n",
    "    #         # if len(part) == 0: continue\n",
    "    #         conj = re.sub(\", \", \"\",matches[idx])\n",
    "    #         parts[idx] = part\n",
    "    #         parts[idx + 1] = conj + parts[idx+1]\n",
    "    #     all_parts.extend(parts)\n",
    "    return_parts = []\n",
    "    merge = False \n",
    "    for part in subsegments:\n",
    "        part = part.strip(\" \")\n",
    "        # if len(part) > 1 and not re.search(r\"^\\)\\s*$|^\\(\\s*$|^[bB]ehold[\\s\\,]*$|^say[\\s\\,]*|^and say[\\s\\,]*|^and you$\",part): \n",
    "        if re.search(r\"say|saith\",part): \n",
    "            # merge with prior part\n",
    "            if len(return_parts) == 0: \n",
    "                merge = True\n",
    "                return_parts.append(part)\n",
    "            else: \n",
    "                return_parts[-1] = return_parts[-1] + \" \" + part  \n",
    "        else: \n",
    "            if merge: \n",
    "                merge = False \n",
    "                return_parts[-1] = return_parts[-1] + \" \" + part\n",
    "            else: \n",
    "                return_parts.append(part) \n",
    "    return return_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_bible = {}\n",
    "for label, verse in bible.items(): \n",
    "    segmented_bible[label] = split_verse(verse)\n",
    "bible_labels = []\n",
    "bible_parts = [] \n",
    "for label, verse in segmented_bible.items(): \n",
    "    for part in verse:\n",
    "        if len(part.split(\" \")) < 2: continue\n",
    "        bible_parts.append(part)\n",
    "        bible_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.0, 21.0, 28.0, 52.0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(s.split(\" \")) for s in bible_parts]\n",
    "import numpy as np \n",
    "np.percentile(lengths,25),np.percentile(lengths,50),np.percentile(lengths,75),np.percentile(lengths,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(norm=None, analyzer='word',sublinear_tf=True)\n",
    "tfidf_bible = vector.fit_transform(bible_parts)\n",
    "df = pd.DataFrame(tfidf_bible.toarray(), columns = vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores = df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average TFIDF score for each verse \n",
    "for idx, score in enumerate(doc_scores): \n",
    "    num_words = len(bible_parts[idx].split(\" \"))\n",
    "    doc_scores[idx] = score/num_words\n",
    "sorted_docs = np.argsort(doc_scores)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/bible/bible_parts.txt','w') as file: \n",
    "    for idx in sorted_docs: \n",
    "        file.write(bible_labels[idx]+\"\\n\")\n",
    "        file.write(bible_parts[idx]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "bible = {}\n",
    "outputfolder = \"/Users/amycweng/DH/SERMONS_APP/db/data\"\n",
    "with open(f'../assets/bible/bible_parts.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    for idx, line in enumerate(lines):\n",
    "      if len(re.findall(\"-\",line)) >= 2 and not re.search(\" \", line.strip(\"\\n\")):\n",
    "        verse = line.strip(\"\\n\")\n",
    "        text = lines[idx+1].strip(\"\\n\").strip(\" \")\n",
    "        # if re.search(r\"^or else$|^yea$|^moreover$|^[lL]o$|^if otherwise$|^wait$|^and yet true$\",text): continue\n",
    "        if verse not in bible: bible[verse] = []\n",
    "        bible[verse].append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36716"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = []\n",
    "for p_list in bible.values(): \n",
    "    unique.extend(p_list)\n",
    "unique = {phrase:idx for idx, phrase in enumerate(set(unique))}\n",
    "unique_bible = {idx:[] for idx in unique.values()}\n",
    "for label, v_list in bible.items():\n",
    "    for verse in v_list:\n",
    "        vidx = unique[verse]\n",
    "        unique_bible[vidx].append(label)\n",
    "phrases_csv = []\n",
    "for phrase, vidx in unique.items(): \n",
    "    phrases_csv.append({\"vidx\":vidx, \"phrase\":phrase})\n",
    "\n",
    "bible_vindices = []\n",
    "for label, vlist in bible.items(): \n",
    "    seen = []\n",
    "    label = re.sub(\"-\",\" \",label).split(\" \")\n",
    "    label = \" \".join(label[:-2]) + \" \" + label[-2] + \":\" + label[-1] + \" (KJV)\"\n",
    "    for v in vlist: \n",
    "        vidx = unique[v]\n",
    "        if vidx in seen: continue\n",
    "        bible_vindices.append({'vidx':vidx, 'verse_id':label})\n",
    "        seen.append(vidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{outputfolder}/bible_phrase_indices.csv\",\"w+\") as outfile: \n",
    "    writer = csv.DictWriter(outfile, fieldnames=bible_vindices[0].keys())\n",
    "    writer.writerows(bible_vindices)\n",
    "with open(f\"{outputfolder}/bible_phrases.csv\",\"w+\") as outfile: \n",
    "    writer = csv.DictWriter(outfile, fieldnames=phrases_csv[0].keys())\n",
    "    writer.writerows(phrases_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
