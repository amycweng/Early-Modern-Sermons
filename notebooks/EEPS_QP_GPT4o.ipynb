{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time \n",
    "import json,math,sys,re\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../\"\n",
    "from dotenv import load_dotenv\n",
    "env_path = f'{folder}/openai.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "OPENAI_API_KEY = os.getenv('SECRET_KEY')\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'verse_id': 'Haggai 2.15 (Geneva) - 1',\n",
       " 'text': 'Our callings, our labours, our actions, and the workes of our hands are sanctified by it, as Psal. 127.',\n",
       " 'verse_text': 'and so are all the workes of their hands, and that which they offer here, is vncleane.',\n",
       " 'label': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputfname = \"Vul_notPriorInCited\"\n",
    "# inputfname = \"preE_Vul\"\n",
    "# inputfname = \"preE_qp\"\n",
    "fname = f\"{inputfname}_GPT4o\"\n",
    "data = pd.read_csv(f\"{folder}/QP/{inputfname}.csv\")\n",
    "data = data.rename(columns={'Unnamed: 0':'index'})\n",
    "data = data.to_dict(orient=\"records\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(data): \n",
    "    item['index'] = idx \n",
    "labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "start = 354 # 0 #\n",
    "end = len(data)\n",
    "my_qp = data[start:end] \n",
    "\n",
    "for qp_pair in tqdm(my_qp):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[ \n",
    "            {\"role\": \"user\",  \n",
    "            #  \"content\": f\"You are an English and Latin scholar working on Biblical language in early modern texts. You determine whether the first sentence is a quotation/paraphrase of a given Biblical verse in the SAME LANGUAGE. You give a single TRUE or FALSE output FOR EACH USER MESSAGE without any reasoning. SENTENCE: '{qp_pair['text']}'; 'VERSE: {qp_pair['verse_text']}'\"}\n",
    "            \"content\":alpaca_prompt.format(\n",
    "            \"You are an English and Latin scholar working on Biblical language in early modern texts. You determine whether the first sentence is a quotation/paraphrase of a given Biblical verse in the SAME LANGUAGE. You give a single TRUE or FALSE output FOR EACH USER MESSAGE without any reasoning.\", # instruction\n",
    "            f\"SENTENCE: '{qp_pair['text']}'; 'VERSE: {qp_pair['verse_text']}'\", # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "            )\n",
    "            }\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "    labels[qp_pair['index']] = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"{folder}/Early-Modern-Sermons/assets/QP_labels/{fname}.json\",'w') as f: \n",
    "    json.dump(labels,f) \n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEPS_QP_assistant = client.beta.assistants.create(\n",
    "  name=\"EEPS Quotations/Paraphrase Identifier\",\n",
    "  instructions=\"You are an English and Latin scholar working on Biblical language in 16th and 17th century texts from England. You can determine whether a given sentence is a quotation/paraphrase of another given Biblical verse in the SAME LANGUAGE. You give TRUE/FALSE outputs FOR EACH USER MESSAGE without any reasoning.\",\n",
    "  model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_QP(qp_pairs):\n",
    "    qp = client.beta.threads.create(\n",
    "        messages=[{\"role\": \"user\", \n",
    "                   \"content\": f\"SENTENCE: '{qp_pair['text']}'; 'VERSE: {qp_pair['verse_text']}'\"} for qp_pair in qp_pairs]\n",
    "    )\n",
    "    return qp.id\n",
    "\n",
    "def run_QP(thread_id):\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread_id,\n",
    "        assistant_id=EEPS_QP_assistant.id\n",
    "    )\n",
    "    return run.id  \n",
    "\n",
    "def wait_for_run_completion(thread_id, run_id):\n",
    "    while True:\n",
    "        run_status = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread_id,\n",
    "            run_id=run_id\n",
    "        )\n",
    "        if run_status.status in [\"completed\", \"failed\", \"cancelled\"]:\n",
    "            return run_status.status\n",
    "        time.sleep(1)  \n",
    "\n",
    "def get_thread_messages(thread_id):\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "    return [(msg.role, msg.content[0].text.value) for msg in messages.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [07:38<00:00,  7.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "start = 555 \n",
    "end = len(data)\n",
    "my_qp = data[start:end] \n",
    "\n",
    "for qp_pair in tqdm(my_qp): \n",
    "    thread_id = add_QP([qp_pair]) \n",
    "    run_id = run_QP(thread_id)  \n",
    "    status = wait_for_run_completion(thread_id, run_id) \n",
    "    if status == \"completed\":\n",
    "        responses = get_thread_messages(thread_id)\n",
    "        for role, text in responses:\n",
    "            if role == 'assistant': \n",
    "                labels[qp_pair['index']] = text \n",
    "    else:\n",
    "        print(f\"Run {qp_pair['index']} failed or was cancelled.\")\n",
    "\n",
    "with open(f\"../../DH/Early-Modern-Sermons/assets/QP_labels/{fname}.json\",'w') as f: \n",
    "    json.dump(labels,f) \n",
    "# takes 30 mins for 250 entries; 11 mins for 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615, 'Vul_notPriorInCited_GPT')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"../../DH/Early-Modern-Sermons/assets/QP_labels/{fname}.json\",'w') as f: \n",
    "    json.dump(labels,f)\n",
    "len(labels),fname"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
