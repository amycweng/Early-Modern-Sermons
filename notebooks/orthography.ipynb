{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271150 known spellings\n"
     ]
    }
   ],
   "source": [
    "import json,math,sys,re,os\n",
    "from tqdm import tqdm \n",
    "sys.path.append('../')\n",
    "\n",
    "from lib.spelling import standard \n",
    "prefixes = ['B','A0','A1','A2','A3','A4','A5','A6','A7','A8','A9']\n",
    "eras = [\"pre-Elizabethan\",\"Elizabethan\",\"Carolinian\",\"Jacobean\",\"CivilWar\",\n",
    "        \"Interregnum\",\"JamesII\",\"CharlesII\",\"WilliamAndMary\"]\n",
    "\n",
    "def get_words(data,type,target_pos=\"all\"): \n",
    "    standardized = {}\n",
    "\n",
    "    for item in data.values(): \n",
    "        if type != \"margin\": \n",
    "            item = item.items()\n",
    "        for i in item: \n",
    "            encodings = i[1]\n",
    "            for token, pos, s in encodings:\n",
    "                token = token.strip(\".\")\n",
    "                pos = pos.strip(\".\")\n",
    "                s = s.strip(\".\")\n",
    "\n",
    "                # proper nouns, abbreviations \n",
    "                if token == pos: continue # no punctuation \n",
    "                if len(s) == 0: continue\n",
    "                if len(pos) == 0: continue \n",
    "                if len(token) == 0: continue\n",
    "                if s[0].islower(): continue\n",
    "                if re.search(\"\\d\",s): continue\n",
    "                if \"NOTE\" in token or \"NONLATINALPHABET\" in token: continue\n",
    "                s = s.lower()\n",
    "\n",
    "                def add_to_standard():\n",
    "                    if s not in standardized: standardized[s] = 0\n",
    "                    standardized[s] += 1\n",
    "                \n",
    "                if target_pos == \"all\": \n",
    "                    if 'fw' not in pos and 'crd' not in pos: \n",
    "                        add_to_standard()\n",
    "                    elif s[0].isupper():\n",
    "                        if ('fw' in pos): add_to_standard() \n",
    "                elif target_pos == \"verbs\": \n",
    "                    if \"v\" in pos:add_to_standard()\n",
    "                elif target_pos == \"nouns\":\n",
    "                    if \"n\" in pos:add_to_standard() \n",
    "                    elif s[0].isupper():\n",
    "                        if ('fw' in pos): add_to_standard() \n",
    "    return standardized\n",
    "\n",
    "def add_to_dict(old,new): \n",
    "    for word, freq in new.items(): \n",
    "        if word not in old: old[word] = freq\n",
    "        else: old[word] += freq\n",
    "    return old\n",
    "\n",
    "\n",
    "def get_vocab(target_era,pos=\"all\"): \n",
    "    standardized = {}\n",
    "    for era in os.listdir(f\"../assets/processed\"):\n",
    "        if era == \".DS_Store\": continue \n",
    "        if era != target_era: continue \n",
    "        print(target_era)\n",
    "        for prefix in os.listdir(f\"../assets/processed/{era}/json\"):\n",
    "            if prefix == \".DS_Store\": continue \n",
    "            if \"_info\" in prefix: continue\n",
    "            print(prefix)\n",
    "            with open(f\"../assets/processed/{era}/json/{prefix}\",\"r\") as file: \n",
    "                data = json.load(file)\n",
    "            if \"_marginalia\" in prefix: \n",
    "                l = get_words(data,\"margin\",pos)\n",
    "                standardized = add_to_dict(standardized,l) \n",
    "            elif \"_text\": \n",
    "                l = get_words(data,\"text\",pos)\n",
    "                standardized = add_to_dict(standardized,l)    \n",
    "\n",
    "        standardized = sorted(standardized.items(), key=lambda x:x[1], reverse=True)\n",
    "        # more than 1 letter; not already a standard spelling; more than half is legible \n",
    "        vocab = [x[0] for x in standardized if len(x[0]) > 1 and x[0] not in standard and (len(re.findall(\"^\",x[0])) < math.floor(len(x[0])/2))]\n",
    "        print(len(vocab),\"words\")\n",
    "\n",
    "        with open(f\"../assets/vocab/{target_era}_{pos}.json\",\"w+\") as file: \n",
    "            json.dump(vocab,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vocab(\"pre-Elizabethan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elizabethan\n",
      "A2_marginalia.json\n",
      "A1_texts.json\n",
      "A6_texts.json\n",
      "B_texts.json\n",
      "B_marginalia.json\n",
      "A0_texts.json\n",
      "A6_marginalia.json\n",
      "A1_marginalia.json\n",
      "A2_texts.json\n",
      "A0_marginalia.json\n",
      "A7_marginalia.json\n",
      "A7_texts.json\n",
      "36854 words\n"
     ]
    }
   ],
   "source": [
    "get_vocab(\"Elizabethan\",\"nouns\") # 37268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "from dotenv import load_dotenv\n",
    "env_path = '/Users/amycweng/DH/openai.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "OPENAI_API_KEY = os.getenv('SECRET_KEY')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def read_words_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = json.load(file)\n",
    "    return words\n",
    "\n",
    "def standardize_spellings(words):\n",
    "    # Prepare the system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant that converts words from Early Modern English sermons to their modern standardized spellings. The input words are separated by newlines. Give the output in the format of original:corrected separated by newlines. Do not add extra white spaces. For example, when I input 'Deut\\nEphes\\nLuk\\nPetecost', I should get 'Deut:Deuteronomy\\nEphes:Ephesians\\nLuk:Luke\\nPetecost:Pentecost' as my output. If the word is in Latin, convert it to English.\"\n",
    "    }\n",
    "\n",
    "    # Prepare the user message with the list of words\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Correct the following words and output the original followed by a colon and the corrected words: \" + \"\\n\".join(words)\n",
    "    }\n",
    "\n",
    "    # Call the OpenAI API with the messages\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[system_message, user_message]\n",
    "    )\n",
    "\n",
    "    # Extract the result from the response\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = {}\n",
    "words = read_words_from_file(\"../assets/vocab/pre-Elizabethan_nouns.json\")\n",
    "words = [w.capitalize() for w in words]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,end = 5000,5557 # 0:100,100:200,200:500,500:1000,1k:1.5k\n",
    "# 1.5k-2k, 2k-2.5k, 2.5k-3k, 3k-3.5k,3.5k-4k,4k-4.5k, 4.5k-5k,5k-finish\n",
    "# I'll -> Ale, Co^ -> Coe, Oxigene -> Oxygen, \n",
    "# Demose -> Demons \n",
    "# 500 words take half a minute to process\n",
    "# Took 10 cents in total    \n",
    "print(len(words[start:end]))\n",
    "standardized_words = standardize_spellings(words[start:end])\n",
    "standardized_words = standardized_words.choices[0].message.content\n",
    "print(standardized_words)\n",
    "standard.update({pair.split(\":\")[0]:pair.split(\":\")[1] for pair in standardized_words.split(\"\\n\") if \":\" in pair})\n",
    "with open(\"../assets/vocab/standard_pre-Elizabethan_nouns.json\",\"w+\") as file: \n",
    "    json.dump(standard,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../assets/vocab/standard_pre-Elizabethan_all.json',\"r\") as file:\n",
    "    standard = json.load(file)\n",
    "fname = \"pre-Elizabethan_all\"\n",
    "words = read_words_from_file(f\"../assets/vocab/{fname}.json\")\n",
    "words = [w.capitalize() for w in words]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thyrdelye:Thirdly\n",
      "Fourthelye:Fourthly\n",
      "Thirdlye:Thirdly\n",
      "Tractent:Treatise\n",
      "^y^^:holy\n",
      "^^^dyous:gracious\n",
      "Diuinit:Divinity\n"
     ]
    }
   ],
   "source": [
    "standardized_words = standardize_spellings(words)\n",
    "standardized_words = standardized_words.choices[0].message.content\n",
    "print(standardized_words)\n",
    "standard.update({pair.split(\":\")[0]:pair.split(\":\")[1] for pair in standardized_words.split(\"\\n\") if \":\" in pair})\n",
    "with open(f\"../assets/vocab/standard_{fname}.json\",\"w+\") as file: \n",
    "    json.dump(standard,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4969\n",
      "432\n",
      "457\n"
     ]
    }
   ],
   "source": [
    "# standardized with GPT 3.5 \n",
    "for fp in os.listdir('../assets/vocab'):\n",
    "    if \"standard\" not in fp: continue\n",
    "    with open(f\"../assets/vocab/{fp}\") as file: \n",
    "        new_standard = json.load(file)\n",
    "    new_standard = {n:k for n,k in new_standard.items() if len(re.findall(\"^\",n)) < math.floor(len(n)/2)}\n",
    "    new_standard = {n:k for n,k in new_standard.items() if n.lower() not in entities and n.lower() not in wordnet_words}\n",
    "    print(len(new_standard))\n",
    "    with open(f\"../assets/vocab/{fp}\",\"w+\") as file: \n",
    "        json.dump(new_standard, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {}\n",
    "for word in words.keys(): \n",
    "    if word[:1] not in prefixes: \n",
    "        prefixes[word[:1]] = []\n",
    "    prefixes[word[:1]].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hits = {}\n",
    "for name in entities:\n",
    "    if name in words: m_hits[name] = words[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prefixes = {}\n",
    "for word in text_words: \n",
    "    if word[:1] not in text_prefixes: \n",
    "        text_prefixes[word[:1]] = []\n",
    "    text_prefixes[word[:1]].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_hits = {}\n",
    "for name in entities:\n",
    "    if name in text_words: t_hits[name] = text_words[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hits_margin = {}\n",
    "for auts in author_ids.values(): \n",
    "    for aut in auts: \n",
    "        aut = aut.split(\" \")\n",
    "        for a in aut: \n",
    "            if len(a.strip(\".\")) < 3: continue\n",
    "            if a in words: \n",
    "                a_hits_margin[a] = words[a]\n",
    "\n",
    "print(len(a_hits_margin))\n",
    "list(sorted(a_hits_margin.items(),key=lambda x:x[1],reverse=True))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(author_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hits_text = {}\n",
    "for auts in author_ids.values(): \n",
    "    for aut in auts: \n",
    "        aut = aut.split(\" \")\n",
    "        for a in aut: \n",
    "            if len(a.strip(\".\")) < 3: continue\n",
    "            if a in text_words: \n",
    "                a_hits_text[a] = text_words[a]\n",
    "print(len(a_hits_text))\n",
    "list(sorted(a_hits_text.items(),key=lambda x:x[1],reverse=True))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/bible_hits.json','w+') as file: \n",
    "    json.dump({'marginal':m_hits, 'in-text':t_hits},file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/author_hits.json','w+') as file: \n",
    "    json.dump({'marginal':a_hits_margin, 'in-text':a_hits_text},file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the most similar words by edit distance to the canonical and biblical hits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "from Levenshtein import distance \n",
    "def dist_fn(s1, s2):\n",
    "    return distance(s1, s2)\n",
    "\n",
    "def find_similar_words(target_word, word_list, threshold,k=20):\n",
    "    similar_words = process.extract(target_word, word_list, limit=None,scorer=dist_fn)\n",
    "    similar_words = [(word, dist) for word, dist in similar_words if dist <= threshold][-k:]\n",
    "    similar_words = sorted(similar_words, key=lambda x:x[1])\n",
    "    return similar_words\n",
    "\n",
    "def find_match(target_word, word_list):\n",
    "    match = process.extract(target_word, word_list, limit=None,scorer=dist_fn)\n",
    "    match = [(word, dist) for word, dist in match if dist == 0]\n",
    "    if len(match) > 0: \n",
    "        return True \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/author_hits.json','r') as file: \n",
    "    a_hits = json.load(file)\n",
    "\n",
    "with open('../assets/bible_hits.json','r') as file: \n",
    "    b_hits = json.load(file)\n",
    "\n",
    "a_hits = a_hits['marginal']\n",
    "b_hits = b_hits['marginal']\n",
    "len(a_hits),len(b_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_spelling(hits):  \n",
    "    for target in hits: \n",
    "        similar_words = find_similar_words(target, text_words.keys(),len(target)/2,10)\n",
    "        print(f\"Words similar to '{target}':\")\n",
    "        for word, dist in similar_words:\n",
    "            print(f\"{word} (Distance: {dist})\")\n",
    "        print()\n",
    "similar_spelling(b_hits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
