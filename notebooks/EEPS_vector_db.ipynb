{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, models \n",
    "import os, torch, re, html, tqdm\n",
    "import pandas as pd \n",
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = html.unescape(text)\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = unidecode(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "def clean_text(s):\n",
    "  s = normalize_text(s)\n",
    "  s = re.sub(r\"</i>|<NOTE>|NONLATINALPHABET|<i>\",\" \",s) # \\d+\\^PAGE[S]*\\^MISSING\"\n",
    "  s = re.sub(r\"\\s+\",\" \",s)\n",
    "  return s.strip(\" \").lower()\n",
    "\n",
    "\n",
    "model_checkpoint = 'emanjavacas/MacBERTh'\n",
    "model_round = \"ALL\"\n",
    "epoch = \"1\"\n",
    "state_dict_path = f\"EEPS_{model_round}_MacBERTh_Epoch{epoch}\"\n",
    "\n",
    "main_dir = '/Users/amycweng/SERMONS_APP/app'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.Transformer(model_checkpoint, max_seq_length=128)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"mean\")\n",
    "bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "bi_encoder.load_state_dict(torch.load(f'{main_dir}/static/data/{state_dict_path}.pt',map_location=torch.device('cpu')))\n",
    "cross_encoder = CrossEncoder(f\"{main_dir}/static/data/EEPS_cross-encoder_emanjavacas_MacBERTh/checkpoint-1000\")\n",
    "basic_encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory=f'{main_dir}/static/data/VECTOR_DB'\n",
    "client_settings = Settings(is_persistent= True, persist_directory= persist_directory, anonymized_telemetry=False)\n",
    "queryclient = chromadb.PersistentClient(path= persist_directory, settings= client_settings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5729"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = \"/Users/amycweng/SERMONS_APP/db/data\"\n",
    "sermons = pd.read_csv(f\"{data_folder}/sermons.csv\",header=None)\n",
    "tcpID_titles = {tcpID:title for tcpID, title in zip(sermons[0],sermons[3])}\n",
    "sermons = pd.read_csv(f\"{data_folder}/sermons_missing.csv\",header=None)\n",
    "tcpID_titles.update({tcpID:title for tcpID, title in zip(sermons[0],sermons[3])})\n",
    "len(tcpID_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Remove 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36702/36702 [00:02<00:00, 15783.42it/s]\n",
      "100%|██████████| 14736/14736 [00:00<00:00, 17677.20it/s]\n",
      "100%|██████████| 31090/31090 [00:01<00:00, 15859.67it/s]\n",
      "100%|██████████| 35811/35811 [00:01<00:00, 24852.96it/s]\n",
      "100%|██████████| 7954/7954 [00:00<00:00, 16134.86it/s]\n",
      "100%|██████████| 9622/9622 [00:00<00:00, 18838.00it/s]\n",
      "100%|██████████| 35809/35809 [00:02<00:00, 12922.86it/s]\n"
     ]
    }
   ],
   "source": [
    "folder = '../../'\n",
    "bible = {}\n",
    "to_remove = {}\n",
    "items = pd.read_csv(f\"{folder}/EEPS/overly_vague.csv\").to_dict(orient='records')\n",
    "for entry in items:\n",
    "  if entry['to_remove'] is True:\n",
    "    to_remove[entry['verse_id']] = None\n",
    "\n",
    "print('To Remove', len(to_remove))\n",
    "\n",
    "b_versions = ['AKJV','ODRV','Geneva', 'Douay-Rheims', 'Tyndale', 'Wycliffe','Vulgate']\n",
    "ODRV_books = pd.read_csv(f\"{folder}/Bibles/ODRV.csv\",header=None)\n",
    "ODRV_books = set(ODRV_books[3])\n",
    "for bname in b_versions:\n",
    "    data = pd.read_csv(f\"{folder}/Bibles/{bname}.csv\",header=None)\n",
    "    data = data.to_dict(orient=\"records\")\n",
    "    for entry in tqdm.tqdm(data):\n",
    "        key = entry[0]\n",
    "        if key in to_remove: continue\n",
    "        v_id = key.split(\" (\")[0]\n",
    "        text = entry[6]\n",
    "        if re.search(\"Douay-Rheims\",key):\n",
    "            if entry[3] in ODRV_books: continue\n",
    "        if len(text.split(\" \")) < 200:\n",
    "            bible[key] = normalize_text(f\"{v_id}: {text}\")\n",
    "\n",
    "        parts = re.split(r'(?<=[\\.\\?]) (?=[A-Z])|(?<=[\\!\\:\\;])', text)\n",
    "        parts = [re.sub(r'\\s+', ' ', p).strip() for p in parts if len(p.strip(\" \")) > 0]\n",
    "        if (len(parts[0].split(\" \")) <= 5 or len(parts[-1].split(\" \")) <= 5 or re.search(r\"\\&\\w+\\;\",parts[0])):\n",
    "            for pidx, p in enumerate(parts): continue\n",
    "        elif len(parts) > 1:\n",
    "            for pidx, p in enumerate(parts):\n",
    "              p_id = f\"{key} - {pidx}\"\n",
    "              if p_id in to_remove: continue\n",
    "              if len(p) == 0: continue\n",
    "              if re.search(r\"\\&\\w+\\;\",p) or len(p.split(\" \")) <= 5: continue\n",
    "              bible[p_id] = normalize_text(f\"Part {pidx+1} of {v_id}: {p}\")\n",
    "\n",
    "bible_verses = list(bible.values())\n",
    "bible_ids = list(bible.keys())\n",
    "\n",
    "bible_vectors = torch.load(f\"{folder}/EEPS/Bibles_{state_dict_path}.pt\",map_location='cpu')\n",
    "bible_vectors = bible_vectors[:-1]\n",
    "\n",
    "full_ids = []\n",
    "full_verses = []\n",
    "full_vectors = []\n",
    "for idx, v_id in enumerate(bible_ids):\n",
    "    if \" - \" not in v_id: # a full verse\n",
    "        full_ids.append(v_id)\n",
    "        full_vectors.append(bible_vectors[idx])\n",
    "        full_verses.append(bible_verses[idx])\n",
    "print(len(full_ids),len(full_vectors),len(full_verses))\n",
    "bible_ids, bible_verses = full_ids, full_verses\n",
    "bible_vectors = full_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "save_full = {v_id: verse for v_id, verse in zip(bible_ids, bible_verses)}\n",
    "with open('/Users/amycweng/SERMONS_APP/app/static/data/full_bibles.json','w+') as file: \n",
    "    json.dump(save_full,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bible'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_collection = queryclient.get_or_create_collection(name=\"Bible\",metadata={\"hnsw:space\": \"cosine\"})\n",
    "bible_collection.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [05:10<00:00, 77.68s/it]\n"
     ]
    }
   ],
   "source": [
    "bible_batches = []\n",
    "batch_size = 40000\n",
    "for i in range(0, len(bible_ids), batch_size):\n",
    "  bible_batches.append((bible_ids[i: i + batch_size],bible_vectors[i: i + batch_size],bible_verses[i:i+batch_size]))\n",
    "for batchids, bvectors,batchtexts in tqdm.tqdm(bible_batches):\n",
    "  bible_collection.upsert(\n",
    "    embeddings=[b.tolist() for b in bvectors],\n",
    "    ids=batchids,\n",
    "    documents=batchtexts\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Galatians 4.30 (ODRV) 0.838 0.673 Galatians 4.30: But what saith the Scripture? Cast out the bond-woman and her sonne. For the sonne of the bond-woman shal not be heire with the sonne of the free-woman.\n"
     ]
    }
   ],
   "source": [
    "neg_sim_threshold = 0.7\n",
    "pos_threshold = 0.6\n",
    "pos_sim_threshold = 0.65\n",
    "neg_threshold = 0.4\n",
    "query = \"the bond-woman and her son were cast out;\"\n",
    "q_embedding = bi_encoder.encode([query])\n",
    "results = bible_collection.query(query_embeddings=q_embedding.tolist(), n_results=25,include=[\"distances\"])\n",
    "for hitlist, distances in zip(results['ids'],results['distances']): \n",
    "    scores = [1 - distances[vidx] for vidx in range(len(hitlist))]\n",
    "    hitlist = [{'v_id':hit, 'score': scores[idx]} for idx, hit in enumerate(hitlist) if scores[idx] >= pos_sim_threshold]\n",
    "    cross_inp = [[query, bible[hit['v_id']]] for hit in hitlist]\n",
    "    if len(cross_inp) == 0: continue\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "    for i in range(len(cross_scores)):\n",
    "        hitlist[i]['cross-score'] = cross_scores[i]\n",
    "    hitlist = sorted(hitlist, key=lambda x: x['cross-score'], reverse=True)\n",
    "\n",
    "    for hit in hitlist:\n",
    "        v_id = hit['v_id']\n",
    "        cross_score = hit['cross-score']\n",
    "        sim_score = hit['score']\n",
    "        if (cross_score >= neg_threshold and sim_score >= neg_sim_threshold) or (cross_score >= pos_threshold):\n",
    "            cross_score = round(cross_score, 3)\n",
    "            sim_score = round(sim_score,3)\n",
    "            print(v_id, cross_score, sim_score, bible[v_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_collection = queryclient.get_or_create_collection(name=\"Titles\",metadata={\"hnsw:space\": \"cosine\"})\n",
    "title_vectors = torch.load(f\"{folder}/EEPS/titles_all-mpnet-base-v2.pt\",map_location='cpu')\n",
    "title_collection.upsert(\n",
    "  embeddings=title_vectors.tolist(),\n",
    "  ids=list(tcpID_titles.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_collection.modify(metadata={\n",
    "        \"hnsw:M\": 256,              # Max connectivity\n",
    "        \"hnsw:ef_construction\": 2000,  # Thorough graph build\n",
    "        \"hnsw:ef\": 10000,           # Search deep enough\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "query = \"marriage\"\n",
    "\n",
    "pos_sim_threshold = 0.35\n",
    "q_embedding = basic_encoder.encode([query])\n",
    "all_data = title_collection.get(include=[\"embeddings\"])\n",
    "all_embeddings = np.array(all_data[\"embeddings\"])  # shape: [5729, dim]\n",
    "all_ids = all_data[\"ids\"]\n",
    "query_embedding = np.array(q_embedding)\n",
    "similarities = np.dot(all_embeddings, query_embedding.T).flatten()\n",
    "sorted_indices = np.argsort(similarities)[::-1]\n",
    "sorted_sim = similarities[sorted_indices]\n",
    "sorted_ids = [all_ids[i] for i in sorted_indices]\n",
    "hitlist = [{'v_id':hit, 'score': score} for hit,score in zip(sorted_ids, sorted_sim) if score >= pos_sim_threshold]\n",
    "cross_inp = [[query, tcpID_titles[hit['v_id']]] for hit in hitlist]\n",
    "if len(cross_inp) > 0:\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "    for i in range(len(cross_scores)):\n",
    "        hitlist[i]['cross-score'] = cross_scores[i]\n",
    "    hitlist = sorted(hitlist, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hitlist:\n",
    "        v_id = hit['v_id']\n",
    "        cross_score = round(hit['cross-score'],3)\n",
    "        sim_score = round(hit['score'],3)\n",
    "        print(v_id, cross_score, sim_score, tcpID_titles[hit['v_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from tqdm import tqdm \n",
    "import math, re\n",
    "import torch\n",
    "def add_to_db(era):\n",
    "  corpus = {} \n",
    "  output = f\"{era}_margin\"\n",
    "  folder = \"/Users/amycweng/DH/Early-Modern-Sermons/assets\"\n",
    "  for fp in tqdm(os.listdir(f\"{folder}/unique\")):\n",
    "      if re.search(era,fp):\n",
    "        if \"margin\" in output:\n",
    "          if not re.search(f'margin',fp): continue\n",
    "        with open(f\"{folder}/unique/{fp}\",\"r\") as file:\n",
    "          r = json.load(file)\n",
    "          for k, v in r.items():\n",
    "            if k not in corpus:\n",
    "              if len(v[0]) == 0: continue\n",
    "              corpus[k] = (v[0],v[1],None)\n",
    "            else:\n",
    "              for loc in v[1]:\n",
    "                corpus[k][1].append(loc)\n",
    "  rel_batches = []\n",
    "  idx_to_p = {}\n",
    "  batch_size = 40000\n",
    "  idx = 0\n",
    "  for i in range(0, len(corpus), batch_size):\n",
    "    batch = []\n",
    "    for p in list(corpus.keys())[i: i + batch_size]:\n",
    "      original = corpus[p][0][0]\n",
    "      idx_to_p[idx] = original\n",
    "      batch.append((idx,original,list(set([tuple(c) for c in corpus[p][1]])),corpus[p][2]))\n",
    "      idx += 1\n",
    "    rel_batches.append(batch)\n",
    "  print(sum([len(v) for v in rel_batches]))\n",
    "\n",
    "  chroma_batches = {}\n",
    "  batch_size = math.ceil(len(corpus)/200000) + 1\n",
    "  batch_num = 0\n",
    "  for i in range(0, len(rel_batches), batch_size):\n",
    "    print(f\"{output}_{batch_num}\")\n",
    "    collection = queryclient.get_or_create_collection(name=f\"{output}_{batch_num}\",metadata={\"hnsw:space\": \"cosine\"})\n",
    "    for j in range(i,i+batch_size):\n",
    "      if j >= len(rel_batches): break\n",
    "      chroma_batches[j] = batch_num\n",
    "    batch_num += 1\n",
    "  print(chroma_batches)\n",
    "\n",
    "  for bidx, batch in enumerate(rel_batches):\n",
    "    p_embedding = torch.load(f\"{data_folder}/embeddings/{output}_{bidx}\",map_location=torch.device('cpu'))\n",
    "    print(len(p_embedding))\n",
    "    cidx = chroma_batches[bidx]\n",
    "    collection = queryclient.get_collection(name=f\"{output}_{cidx}\")\n",
    "    print(collection)\n",
    "    docs = [\";\".join([\"_\".join(key) for key in b[2]]) for b in batch]\n",
    "    collection.upsert(\n",
    "      embeddings=p_embedding.tolist(),\n",
    "      ids=[str(b[0]) for b in batch],\n",
    "      documents= docs\n",
    "    )\n",
    "    print(f\"finished inserting to my Chroma collection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sermons_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
