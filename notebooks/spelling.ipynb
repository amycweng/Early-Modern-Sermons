{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,math,sys,re,os\n",
    "from tqdm import tqdm \n",
    "sys.path.append('../')\n",
    "\n",
    "from lib.spelling import standard,standardizer \n",
    "print(len(standard),\"known spellings\")\n",
    "\n",
    "prefixes = ['B','A0','A1','A2','A3','A4','A5','A6','A7','A8','A9']\n",
    "eras = [\"pre-Elizabethan\",\"Elizabethan\",\"Carolinian\",\"Jacobean\",\"CivilWar\",\n",
    "        \"Interregnum\",\"JamesII\",\"CharlesII\",\"WilliamAndMary\"]\n",
    "         \n",
    "def get_words(data,type,target_pos=\"all\"): \n",
    "    standardized = {}\n",
    "\n",
    "    for item in data.values(): \n",
    "        if type != \"margin\": \n",
    "            item = item.items()\n",
    "        for i in item: \n",
    "            encodings = i[1]\n",
    "            for token, pos, s in encodings:\n",
    "                token = token.strip(\".\")\n",
    "                pos = pos.strip(\".\")\n",
    "                s = s.strip(\".\")\n",
    "\n",
    "                # proper nouns, abbreviations \n",
    "                if token == pos: continue # no punctuation \n",
    "                if len(s) == 0: continue\n",
    "                if len(pos) == 0: continue \n",
    "                if len(token) == 0: continue\n",
    "                if s[0].islower(): continue\n",
    "                if re.search(\"\\d\",s): continue\n",
    "                if \"NOTE\" in token or \"NONLATINALPHABET\" in token: continue\n",
    "                s = s.lower()\n",
    "\n",
    "                def add_to_standard():\n",
    "                    if s not in standardized: standardized[s] = 0\n",
    "                    standardized[s] += 1\n",
    "                \n",
    "                if target_pos == \"all\": \n",
    "                    if 'fw' not in pos and 'crd' not in pos: \n",
    "                        add_to_standard()\n",
    "                    elif s[0].isupper():\n",
    "                        if ('fw' in pos): add_to_standard() \n",
    "                elif target_pos == \"verbs\": \n",
    "                    if \"v\" in pos:add_to_standard()\n",
    "                elif target_pos == \"nouns\":\n",
    "                    if \"np\" in pos:add_to_standard() \n",
    "                    elif s[0].isupper():\n",
    "                        if ('n' in pos) or ('fw' in pos): add_to_standard() \n",
    "    return standardized\n",
    "\n",
    "def add_to_dict(old,new): \n",
    "    for word, freq in new.items():\n",
    "        if len(word) < 2: continue \n",
    "        if word not in old: old[word] = freq\n",
    "        else: old[word] += freq\n",
    "    return old   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = {}\n",
    "pos = \"all\"\n",
    "for era in os.listdir(f\"../assets/processed\"):\n",
    "    if era == \".DS_Store\": continue \n",
    "\n",
    "    print(era)\n",
    "    for prefix in os.listdir(f\"../assets/processed/{era}/json\"):\n",
    "        if prefix == \".DS_Store\": continue \n",
    "        if \"_info\" in prefix: continue\n",
    "        print(prefix)\n",
    "        with open(f\"../assets/processed/{era}/json/{prefix}\",\"r\") as file: \n",
    "            data = json.load(file)\n",
    "        if \"_marginalia\" in prefix: \n",
    "            l = get_words(data,\"margin\",pos)\n",
    "            standardized = add_to_dict(standardized,l) \n",
    "        elif \"_text\" in prefix: \n",
    "            l = get_words(data,\"text\",pos)\n",
    "            standardized = add_to_dict(standardized,l) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more than 1 letter; not already a standard spelling; more than half is legible \n",
    "# more than five occurrences \n",
    "vocab_all = sorted(standardized.items(), key=lambda x:x[1], reverse=True)\n",
    "vocab_all = [x for x in vocab_all if x[1] > 0 and x[0] not in standard and (len(re.findall(\"\\^\",x[0])) < math.floor(len(x[0])/2)) and re.sub(r\"s$|\\'s$|\\!|\\?|\\:\",\"\",x[0]) not in standard]\n",
    "vocab = [x[0] for x in vocab_all if x[1] > 1]\n",
    "print(len(vocab),\"words to standardize out of\",len(vocab_all))\n",
    "\n",
    "with open(f\"../assets/vocab/{pos}.json\",\"w+\") as file: \n",
    "    json.dump(vocab_all,file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [v[1] for v in vocab_all if v[1] > 1]\n",
    "import numpy as np\n",
    "for p in [10,25,50,75]: \n",
    "    print(p,\": \", np.percentile(counts,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "from dotenv import load_dotenv\n",
    "env_path = '../../DH/openai.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "OPENAI_API_KEY = os.getenv('SECRET_KEY')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def read_words_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = json.load(file)\n",
    "    return words\n",
    "\n",
    "def standardize_spellings(words):\n",
    "    # Prepare the system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant that converts words from Early Modern English sermons to their modern standardized spellings.\"\n",
    "    }\n",
    "\n",
    "    # Prepare the user message with the list of words\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Translate the following words to English in the format of original:translation separated by newlines. Do not add extra white spaces, and keep carets ^ in the original if it occurs. For example, when I input 'Deut^\\nEphes\\nThes^\\nPetecost\\nWollebii\\nGr^tius', I should get 'Deut^:Deuteronomy\\nEphes:Ephesians\\Thes^:Thessalonians\\nPetecost:Pentecost\\nWollebii:Wolleb\\nGr^tius:Grotius' as my output. Translate the following words to English: \" + \"\\n\".join(words)\n",
    "    }\n",
    "\n",
    "    # Call the OpenAI API with the messages\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[system_message, user_message]\n",
    "    )\n",
    "\n",
    "    # Extract the result from the response\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"all.json\"\n",
    "words = read_words_from_file(f\"../assets/vocab/{fname}\")\n",
    "words = [x[0] for x in words if x[1] > 1 and x[0] not in standard]\n",
    "words = [w.capitalize() for w in words if (len(re.findall(\"\\^\",w)) < math.floor(len(w)/2))]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 20000 #\n",
    "outputfname = \"all_greater_than_4.json\"\n",
    "while start < len(words): \n",
    "    end = start + 500\n",
    "    print(start,end,words[start])\n",
    "\n",
    "    if start > 0: \n",
    "        with open(f\"../assets/vocab/standard_{outputfname}\",\"r\") as file: \n",
    "            new_standard = json.load(file)\n",
    "    else: \n",
    "        new_standard = {}\n",
    "    standardized_words = standardize_spellings(words[start:end])\n",
    "    standardized_words = standardized_words.choices[0].message.content\n",
    "    print(standardized_words)\n",
    "    new_standard.update({pair.split(\":\")[0]:pair.split(\":\")[1] for pair in standardized_words.split(\"\\n\") if \":\" in pair})\n",
    "    with open(f\"../assets/vocab/standard_{outputfname}\",\"w+\") as file: \n",
    "        json.dump(new_standard,file)\n",
    "    start = end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblical Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the most similar words by edit distance to the biblical hits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "from Levenshtein import distance \n",
    "def dist_fn(s1, s2):\n",
    "    return distance(s1, s2)\n",
    "\n",
    "def find_similar_words(target_word, word_list, threshold,k=20):\n",
    "    similar_words = process.extract(target_word, word_list, limit=None,scorer=dist_fn)\n",
    "    similar_words = [(word, dist) for word, dist in similar_words if dist <= threshold][-k:]\n",
    "    similar_words = sorted(similar_words, key=lambda x:x[1])\n",
    "    return similar_words\n",
    "\n",
    "def find_match(target_word, word_list):\n",
    "    match = process.extract(target_word, word_list, limit=None,scorer=dist_fn)\n",
    "    match = [(word, dist) for word, dist in match if dist == 0]\n",
    "    if len(match) > 0: \n",
    "        return True \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.spelling import entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_spelling(hits):  \n",
    "    for target in hits: \n",
    "        similar_words = find_similar_words(target,words,len(target)/2,10)\n",
    "        print(f\"Words similar to '{target}':\")\n",
    "        for word, dist in similar_words:\n",
    "            print(f\"{word} (Distance: {dist})\")\n",
    "        print()\n",
    "similar_spelling(entities[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
