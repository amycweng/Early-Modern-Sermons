{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274075 known spellings\n",
      "8689 corrected spellings\n",
      "274075 known spellings\n"
     ]
    }
   ],
   "source": [
    "import json,math,sys,re,os\n",
    "from tqdm import tqdm \n",
    "sys.path.append('../')\n",
    "\n",
    "from lib.spelling import standard,standardizer \n",
    "print(len(standardizer),\"corrected spellings\")\n",
    "print(len(standard),\"known spellings\")\n",
    "\n",
    "prefixes = ['B','A0','A1','A2','A3','A4','A5','A6','A7','A8','A9']\n",
    "eras = [\"pre-Elizabethan\",\"Elizabethan\",\"Carolinian\",\"Jacobean\",\"CivilWar\",\n",
    "        \"Interregnum\",\"JamesII\",\"CharlesII\",\"WilliamAndMary\"]\n",
    "         \n",
    "def get_words(data,type,target_pos=\"all\"): \n",
    "    standardized = {}\n",
    "\n",
    "    for item in data.values(): \n",
    "        if type != \"margin\": \n",
    "            item = item.items()\n",
    "        for i in item: \n",
    "            encodings = i[1]\n",
    "            for token, pos, s in encodings:\n",
    "                token = token.strip(\".\")\n",
    "                pos = pos.strip(\".\")\n",
    "                s = s.strip(\".\")\n",
    "\n",
    "                # proper nouns, abbreviations \n",
    "                if token == pos: continue # no punctuation \n",
    "                if len(s) == 0: continue\n",
    "                if len(pos) == 0: continue \n",
    "                if len(token) == 0: continue\n",
    "                if s[0].islower(): continue\n",
    "                if re.search(\"\\d\",s): continue\n",
    "                if \"NOTE\" in token or \"NONLATINALPHABET\" in token: continue\n",
    "                s = s.lower()\n",
    "\n",
    "                def add_to_standard():\n",
    "                    if s not in standardized: standardized[s] = 0\n",
    "                    standardized[s] += 1\n",
    "                \n",
    "                if target_pos == \"all\": \n",
    "                    if 'fw' not in pos and 'crd' not in pos: \n",
    "                        add_to_standard()\n",
    "                    elif s[0].isupper():\n",
    "                        if ('fw' in pos): add_to_standard() \n",
    "                elif target_pos == \"verbs\": \n",
    "                    if \"v\" in pos:add_to_standard()\n",
    "                elif target_pos == \"nouns\":\n",
    "                    if \"n\" in pos:add_to_standard() \n",
    "                    elif s[0].isupper():\n",
    "                        if ('fw' in pos): add_to_standard() \n",
    "    return standardized\n",
    "\n",
    "def add_to_dict(old,new): \n",
    "    for word, freq in new.items(): \n",
    "        if word not in old: old[word] = freq\n",
    "        else: old[word] += freq\n",
    "    return old\n",
    "\n",
    "\n",
    "def get_vocab(target_era,pos=\"all\"): \n",
    "    standardized = {}\n",
    "    for era in os.listdir(f\"../assets/processed\"):\n",
    "        if era == \".DS_Store\": continue \n",
    "        if era != target_era: continue \n",
    "        print(target_era)\n",
    "        for prefix in os.listdir(f\"../assets/processed/{era}/json\"):\n",
    "            if prefix == \".DS_Store\": continue \n",
    "            if \"_info\" in prefix: continue\n",
    "            print(prefix)\n",
    "            with open(f\"../assets/processed/{era}/json/{prefix}\",\"r\") as file: \n",
    "                data = json.load(file)\n",
    "            if \"_marginalia\" in prefix: \n",
    "                l = get_words(data,\"margin\",pos)\n",
    "                standardized = add_to_dict(standardized,l) \n",
    "            elif \"_text\" in prefix: \n",
    "                l = get_words(data,\"text\",pos)\n",
    "                standardized = add_to_dict(standardized,l)    \n",
    "\n",
    "        standardized = sorted(standardized.items(), key=lambda x:x[1], reverse=True)\n",
    "        # more than 1 letter; not already a standard spelling; more than half is legible \n",
    "        # more than one occurrences \n",
    "        vocab = [x[0] for x in standardized if x[1] > 1 and len(x[0]) > 1 and x[0] not in standard and (len(re.findall(\"^\",x[0])) < math.floor(len(x[0])/2))]\n",
    "        vocab = [x for x in vocab if re.sub(r\"s$|\\'s$\",\"\",x) not in standard]\n",
    "        print(len(vocab),\"words\")\n",
    "\n",
    "        with open(f\"../assets/vocab/{target_era}_{pos}.json\",\"w+\") as file: \n",
    "            json.dump(vocab,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vocab(\"CivilWar\") # nouns: 5461 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "from dotenv import load_dotenv\n",
    "env_path = '/Users/amycweng/DH/openai.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "OPENAI_API_KEY = os.getenv('SECRET_KEY')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def read_words_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = json.load(file)\n",
    "    return words\n",
    "\n",
    "def standardize_spellings(words):\n",
    "    # Prepare the system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant that converts words from Early Modern English sermons to their modern standardized spellings.\"\n",
    "    }\n",
    "\n",
    "    # Prepare the user message with the list of words\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Translate the following words to English in the format of original:corrected separated by newlines. Do not add extra white spaces. For example, when I input 'Deut\\nEphes\\nLuk\\nPetecost\\nQuidque', I should get 'Deut:Deuteronomy\\nEphes:Ephesians\\nLuk:Luke\\nPetecost:Pentecost' as my output. Translate the following words to English: \" + \"\\n\".join(words)\n",
    "    }\n",
    "\n",
    "    # Call the OpenAI API with the messages\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[system_message, user_message]\n",
    "    )\n",
    "\n",
    "    # Extract the result from the response\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5461"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard = {}\n",
    "fname = \"CivilWar_nouns.json\"\n",
    "words = read_words_from_file(f\"../assets/vocab/{fname}\")\n",
    "words = [w.capitalize() for w in words]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n",
      "5200 5462 ['Pelagianae', 'Isayah', 'Premissa', 'Legitima', 'Gun-powder-treason', 'Seribes', 'Renisus', 'Senecae', 'Philocrates', 'Prisonhouse', 'Hormageddon', 'Quotidiana', 'Kirke', 'Godto', 'Ribrlath', 'Sallamanders', 'Marii', 'Salsedo', \"Chasma's\", 'Chorahs', 'Armato', 'Absynthium', 'Rodani', 'Ithacus', 'Chasmas', 'Law-curse', 'Law-debt', 'Ev^ngelists', 'Mintmaster', 'State-wits', 'State-wisdom', 'Bright-star', 'Sen^^s', 'Law-work', 'Templ^', 'Standart', 'Church-ordinances', 'Th^^e', 'Positi', 'Ilanders', 'Grammattications', 'Dordr', 'Jehojachim', 'A^minians', 'Objct', 'Gospel-sinne', 'Gospel-vengeance', 'Grevinchovius', 'Master-wheel', 'Coronation-day', 'Po^ts', 'N^tions', 'Obj^ct', 'Lame-pot', 'Iehaziel', 'Tehillah', 'Tephillah', 'Alchymistry', 'Iehiel', 'Wo^d', 'M^^isters', 'Scripture-phrase', 'Goddesses', 'Plimouth', 'Shadrack', 'Mar^yr', 'Caluin', 'Archbishop^', 'Barsabas', 'Gos^el', 'Penry', 'Barnaba^', 'Geneua', 'N^ah', 'Giffords', 'The^', 'Pap^sts', 'H^zekiah', 'Vitalian', 'Calistus', 'Crashawes', 'Crashawe', 'Balaamites', 'Rabbine', 'Sweede', 'Wo^ship', 'Propet', 'Phifitian', 'Lahan', 'Incouragemet', 'Scevola', 'Punicks', 'Cavalierisme', 'Crancius', 'Apostolicum', 'Alio', 'Provinciales', 'Netherland', 'Maiestati', 'Gygantum', 'Lucif', 'Scripture-reason', 'Ecclesia^', 'Bruti', 'Diodat', 'Ire^', 'Ibi^', 'Duraeum', 'Aurel', 'Impp', 'Confirmat', 'Prioresses', 'Oxon', 'Cassiod', 'Incouragment', 'Etsi', 'Danmonii', 'M^cah', 'Marleborough', 'Blessensis', 'Rodulph', 'Applyca', 'Newfishstreet', 'Imperatorum', 'Intreduction', 'Ezia', 'Appl^c', 'Are^', 'Secessio', 'Organicum', 'Injusta', 'Infan^s', 'Fernel', 'Pulpit-jars', 'Minister-worship', 'Myrias', 'Traditiones', 'Ludolphus', 'Scripturelight', 'Ephos', 'Honorantes', 'Sen^^', 'Clapmar', 'Dictatores', 'Valeria', 'Turneb', 'Consulum', 'Emendatores', 'Albutius', 'Bellarminus', 'Ecclesi^', 'Alliaco', 'Africano', 'Gregori^', 'Origenis', 'Whittak', 'Pelagus', 'Commandmets', 'Hexam', 'Trallens', 'Cyri', 'Gallicis', 'Pamph', 'Nem^', 'Honoris', 'Arianorum', 'Aethiopes', 'Covevenant', 'B^za', 'Namb', 'Purch', 'Parliamentmen', 'Ribad', 'Ribadin', 'Lucens', 'Ansel', 'Pellic', 'Innius', 'Chrono', 'Isa^', 'Balsack', 'Marcelli', 'Nodite', 'Ortel', 'Na^ion', 'Ramases', 'Bartimens', 'Icrusalem', 'Finchingfield', 'Isra^l', 'Bible-readers', 'Zalmanah', 'Lo^^', 'Conjunctim', 'Nabuchodonozor', 'Cavaliert', 'Marleborow', 'Scripture-truths', 'Thamesis', 'Dration', 'Deifide', 'Habington', 'Caire', 'Brudenol', 'Gaurus', 'Mustered-seed', 'Powder-plots', 'Pembrook', 'Cstrist', 'Cothians', 'Willoughby', 'Scroop', 'Iervis', 'Salkeild', 'Collingvvood', 'Carlton', 'Gehazies', 'Club-men', 'Archilocus', 'Lycambes', 'Mytre', 'Sea-dangers', 'Idoniety', 'Cverplus', 'Supernaturallity', 'Aboundin', 'Johah', 'Free-justification', 'Inlargment', 'Sau^', 'Ismaell', 'Church-agreement', 'Areae', 'Church-corruptions', 'Isaies', 'Foundation-stones', 'Zerub^abel', 'Abrenuncio', 'Civil-wars', 'Blazing-star', 'Expositer', 'Paroth', 'Church-society', 'Church-establishment', 'Southwarke', 'Acrosticke', 'Cambels', 'N^mrod', 'Beraeans', 'Covena^t', 'Corrigibilitie', 'Joth^m', 'Iesaiam', 'Shirion', 'Family-reformation', 'Ennoblishment', 'Ignicles', 'Sceptiques', 'Houshold-godlinesse', 'Libellatici', 'Set-form'] \n",
      " Pelagianae:Pelagian\n",
      "Isayah:Isaiah\n",
      "Premissa:Premise\n",
      "Legitima:Legitimate\n",
      "Gun-powder-treason:Gunpowder treason\n",
      "Seribes:Sermons\n",
      "Renisus:Renewal\n",
      "Senecae:Seneca\n",
      "Philocrates:Philocrates\n",
      "Prisonhouse:Prison house\n",
      "Hormageddon:Armageddon\n",
      "Quotidiana:Quotidian\n",
      "Kirke:Church\n",
      "Godto:God\n",
      "Ribrlath:Riblath\n",
      "Sallamanders:Salamanders\n",
      "Marii:Mary\n",
      "Salsedo:Salted\n",
      "Chasma's:Chasmas\n",
      "Chorahs:Choruses\n",
      "Armato:Armato\n",
      "Absynthium:Absinthe\n",
      "Rodani:Rhodani\n",
      "Ithacus:Ithaca\n",
      "Chasmas:Chasms\n",
      "Law-curse:Law curse\n",
      "Law-debt:Law debt\n",
      "Ev^ngelists:Evangelists\n",
      "Mintmaster:Mint master\n",
      "State-wits:State wits\n",
      "State-wisdom:State wisdom\n",
      "Bright-star:Bright star\n",
      "Sen^^s:Seneca\n",
      "Law-work:Law work\n",
      "Templ^:Temple\n",
      "Standart:Standard\n",
      "Church-ordinances:Church ordinances\n",
      "Th^^e:Thee\n",
      "Positi:Position\n",
      "Ilanders:Islanders\n",
      "Grammattications:Grammatications\n",
      "Dordr:Dordrecht\n",
      "Jehojachim:Jehoiachim\n",
      "A^minians:Arminians\n",
      "Objct:Object\n",
      "Gospel-sinne:Gospel sin\n",
      "Gospel-vengeance:Gospel vengeance\n",
      "Grevinchovius:Grevinchovius\n",
      "Master-wheel:Master wheel\n",
      "Coronation-day:Coronation day\n",
      "Po^ts:Poets\n",
      "N^tions:Nations\n",
      "Obj^ct:Object\n",
      "Lame-pot:Lame pot\n",
      "Iehaziel:Jehaziel\n",
      "Tehillah:Tehillah\n",
      "Tephillah:Tephillah\n",
      "Alchymistry:Alchemy\n",
      "Iehiel:Jehiel\n",
      "Wo^d:Word\n",
      "M^^isters:Ministers\n",
      "Scripture-phrase:Scripture phrase\n",
      "Goddesses:Goddesses\n",
      "Plimouth:Plymouth\n",
      "Shadrack:Shadrach\n",
      "Mar^yr:Martyr\n",
      "Caluin:Calvin\n",
      "Archbishop^:Archbishop\n",
      "Barsabas:Barsabbas\n",
      "Gos^el:Gospel\n",
      "Penry:Penry\n",
      "Barnaba^:Barnabas\n",
      "Geneua:Geneva\n",
      "N^ah:Nah\n",
      "Giffords:Giffords\n",
      "The^:The\n",
      "Pap^sts:Papists\n",
      "H^zekiah:Hezekiah\n",
      "Vitalian:Vitalian\n",
      "Calistus:Calistus\n",
      "Crashawes:Crashaws\n",
      "Crashawe:Crashaw\n",
      "Balaamites:Balaamites\n",
      "Rabbine:Rabbis\n",
      "Sweede:Swede\n",
      "Wo^ship:Worship\n",
      "Propet:Prophet\n",
      "Phifitian:Physician\n",
      "Lahan:Lahan\n",
      "Incouragemet:Encouragement\n",
      "Scevola:Scevola\n",
      "Punicks:Punics\n",
      "Cavalierisme:Cavalierism\n",
      "Crancius:Crancius\n",
      "Apostolicum:Apostolicum\n",
      "Alio:Alio\n",
      "Provinciales:Provinciales\n",
      "Netherland:Netherlands\n",
      "Maiestati:Majesty\n",
      "Gygantum:Giants\n",
      "Lucif:Lucifer\n",
      "Scripture-reason:Scriptural reason\n",
      "Ecclesia^:Ecclesia\n",
      "Bruti:Bruti\n",
      "Diodat:Diodati\n",
      "Ire^:Irene\n",
      "Ibi^:Ibiza\n",
      "Duraeum:Duraeum\n",
      "Aurel:Aurel\n",
      "Impp:Imperatorum\n",
      "Confirmat:Confirmed\n",
      "Prioresses:Prioresses\n",
      "Oxon:Oxford\n",
      "Cassiod:Cassiodorus\n",
      "Incouragment:Encouragement\n",
      "Etsi:Though\n",
      "Danmonii:Damonii\n",
      "M^cah:Micha\n",
      "Marleborough:Marlborough\n",
      "Blessensis:Blessensis\n",
      "Rodulph:Rudolph\n",
      "Applyca:Applyca\n",
      "Newfishstreet:New Fish Street\n",
      "Imperatorum:Emperor\n",
      "Intreduction:Introduction\n",
      "Ezia:Hezekiah\n",
      "Appl^c:Apply\n",
      "Are^:Area\n",
      "Secessio:Secessio\n",
      "Organicum:Organic\n",
      "Injusta:Unjust\n",
      "Infan^s:Infants\n",
      "Fernel:Fernel\n",
      "Pulpit-jars:Pulpit jars\n",
      "Minister-worship:Minister worship\n",
      "Myrias:Myriads\n",
      "Traditiones:Traditions\n",
      "Ludolphus:Ludolphus\n",
      "Scripturelight:Scriptural light\n",
      "Ephos:Ephesus\n",
      "Honorantes:Honoring\n",
      "Sen^^s:Seneca\n",
      "Clapmar:Clapmar\n",
      "Dictatores:Dictators\n",
      "Valeria:Valeria\n",
      "Turneb:Turnebus\n",
      "Consulum:Consulum\n",
      "Emendatores:Emendatores\n",
      "Albutius:Albutius\n",
      "Bellarminus:Bellarminus\n",
      "Ecclesi^:Ecclesiastical\n",
      "Alliaco:Alliaco\n",
      "Africano:Africano\n",
      "Gregori^:Gregory\n",
      "Origenis:Origen's\n",
      "Whittak:Whittaker\n",
      "Pelagus:Plague\n",
      "Commandmets:Commandments\n",
      "Hexam:Hexam\n",
      "Trallens:Trallens\n",
      "Cyri:Cyri\n",
      "Gallicis:Gallic\n",
      "Pamph:Pamphlet\n",
      "Nem^:Nemesis\n",
      "Honoris:Honoris\n",
      "Arianorum:Arianorum\n",
      "Aethiopes:Ethiopians\n",
      "Covevenant:Covenant\n",
      "B^za:Beza\n",
      "Namb:Numb\n",
      "Purch:Purchase\n",
      "Parliamentmen:Parliament men\n",
      "Ribad:Ribald\n",
      "Ribadin:Ribald\n",
      "Lucens:Lucens\n",
      "Ansel:Ansel\n",
      "Pellic:Pellic\n",
      "Innius:Inn\n",
      "Chrono:Chrono\n",
      "Isa^:Isaiah\n",
      "Balsack:Balsack\n",
      "Marcelli:Marcelli\n",
      "Nodite:Nodite\n",
      "Ortel:Ortel\n",
      "Na^ion:Nation\n",
      "Ramases:Ramses\n",
      "Bartimens:Bartimens\n",
      "Icrusalem:Jerusalem\n",
      "Finchingfield:Finchingfield\n",
      "Isra^l:Israel\n",
      "Bible-readers:Bible readers\n",
      "Zalmanah:Zalmanah\n",
      "Lo^^:Love\n",
      "Conjunctim:Conjointly\n",
      "Nabuchodonozor:Nebuchadnezzar\n",
      "Cavaliert:Cavalier\n",
      "Marleborow:Marlborough\n",
      "Scripture-truths:Scriptural truths\n",
      "Thamesis:Thames\n",
      "Dration:Duration\n",
      "Deifide:Deified\n",
      "Habington:Habington\n",
      "Caire:Cairo\n",
      "Brudenol:Brudenol\n",
      "Gaurus:Gaurus\n",
      "Mustered-seed:Mustard seed\n",
      "Powder-plots:Gunpowder plots\n",
      "Pembrook:Pembroke\n",
      "Cstrist:Christ\n",
      "Cothians:Corinthians\n",
      "Willoughby:Willoughby\n",
      "Scroop:Scroop\n",
      "Iervis:Jervis\n",
      "Salkeild:Salkeild\n",
      "Collingvvood:Collingwood\n",
      "Carlton:Carlton\n",
      "Gehazies:Gehazies\n",
      "Club-men:Club men\n",
      "Archilocus:Archilocus\n",
      "Lycambes:Lycambes\n",
      "Mytre:Mitre\n",
      "Sea-dangers:Seadangers\n",
      "Idoniety:Idolatry\n",
      "Cverplus:Overplus\n",
      "Supernaturallity:Supernaturallity\n",
      "Aboundin:Abounding\n",
      "Johah:Jonah\n",
      "Free-justification:Free justification\n",
      "Inlargment:Enlargement\n",
      "Sau^:Saul\n",
      "Ismaell:Ishmael\n",
      "Church-agreement:Church agreement\n",
      "Areae:Area\n",
      "Church-corruptions:Church corruptions\n",
      "Isaies:Isaiah\n",
      "Foundation-stones:Foundation stones\n",
      "Zerub^abel:Zerubbabel\n",
      "Abrenuncio:Abrenuncio\n",
      "Civil-wars:Civil wars\n",
      "Blazing-star:Blazing star\n",
      "Expositer:Expositor\n",
      "Paroth:Paroth\n",
      "Church-society:Church society\n",
      "Church-establishment:Church establishment\n",
      "Southwarke:Southwark\n",
      "Acrosticke:Acrostic\n",
      "Cambels:Camels\n",
      "N^mrod:Nimrod\n",
      "Beraeans:Bereans\n",
      "Covena^t:Covenant\n",
      "Corrigibilitie:Corrigibility\n",
      "Joth^m:Jotham\n",
      "Iesaiam:Isaiah\n",
      "Shirion:Shirion\n",
      "Family-reformation:Family reformation\n",
      "Ennoblishment:Ennoblement\n",
      "Ignicles:Ignacles\n",
      "Sceptiques:Skeptics\n",
      "Houshold-godlinesse:Household godliness\n",
      "Libellatici:Libellatici\n",
      "Set-form:Set form\n"
     ]
    }
   ],
   "source": [
    "start,end = 5200,5462 # 4.3k\n",
    "print(len(words[start:end]))\n",
    "if start > 0: \n",
    "    with open(f\"../assets/vocab/standard_{fname}\",\"r\") as file: \n",
    "        new_standard = json.load(file)\n",
    "else: \n",
    "    new_standard = {}\n",
    "standardized_words = standardize_spellings(words[start:end])\n",
    "standardized_words = standardized_words.choices[0].message.content\n",
    "print(start,end,words[start:end],'\\n',standardized_words)\n",
    "new_standard.update({pair.split(\":\")[0]:pair.split(\":\")[1] for pair in standardized_words.split(\"\\n\") if \":\" in pair})\n",
    "with open(f\"../assets/vocab/standard_{fname}\",\"w+\") as file: \n",
    "    json.dump(new_standard,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preE \n",
    "# 0:100,100:200,200:500,500:1000,1k:1.5k\n",
    "# 1.5k-2k, 2k-2.5k, 2.5k-3k, 3k-3.5k,3.5k-4k,4k-4.5k, 4.5k-5k,5k-finish\n",
    "# I'll -> Ale, Co^ -> Coe, Oxigene -> Oxygen, \n",
    "# Demose -> Demons \n",
    "# 500 words take half a minute to process\n",
    "# Took 10 cents in total    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../assets/vocab/standard_pre-Elizabethan_all.json',\"r\") as file:\n",
    "    standard = json.load(file)\n",
    "fname = \"pre-Elizabethan_all\"\n",
    "words = read_words_from_file(f\"../assets/vocab/{fname}.json\")\n",
    "words = [w.capitalize() for w in words]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thyrdelye:Thirdly\n",
      "Fourthelye:Fourthly\n",
      "Thirdlye:Thirdly\n",
      "Tractent:Treatise\n",
      "^y^^:holy\n",
      "^^^dyous:gracious\n",
      "Diuinit:Divinity\n"
     ]
    }
   ],
   "source": [
    "standardized_words = standardize_spellings(words)\n",
    "standardized_words = standardized_words.choices[0].message.content\n",
    "print(standardized_words)\n",
    "standard.update({pair.split(\":\")[0]:pair.split(\":\")[1] for pair in standardized_words.split(\"\\n\") if \":\" in pair})\n",
    "with open(f\"../assets/vocab/standard_{fname}.json\",\"w+\") as file: \n",
    "    json.dump(standard,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblical & Classical Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {}\n",
    "for word in words.keys(): \n",
    "    if word[:1] not in prefixes: \n",
    "        prefixes[word[:1]] = []\n",
    "    prefixes[word[:1]].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hits = {}\n",
    "for name in entities:\n",
    "    if name in words: m_hits[name] = words[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prefixes = {}\n",
    "for word in text_words: \n",
    "    if word[:1] not in text_prefixes: \n",
    "        text_prefixes[word[:1]] = []\n",
    "    text_prefixes[word[:1]].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_hits = {}\n",
    "for name in entities:\n",
    "    if name in text_words: t_hits[name] = text_words[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hits_margin = {}\n",
    "for auts in author_ids.values(): \n",
    "    for aut in auts: \n",
    "        aut = aut.split(\" \")\n",
    "        for a in aut: \n",
    "            if len(a.strip(\".\")) < 3: continue\n",
    "            if a in words: \n",
    "                a_hits_margin[a] = words[a]\n",
    "\n",
    "print(len(a_hits_margin))\n",
    "list(sorted(a_hits_margin.items(),key=lambda x:x[1],reverse=True))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(author_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hits_text = {}\n",
    "for auts in author_ids.values(): \n",
    "    for aut in auts: \n",
    "        aut = aut.split(\" \")\n",
    "        for a in aut: \n",
    "            if len(a.strip(\".\")) < 3: continue\n",
    "            if a in text_words: \n",
    "                a_hits_text[a] = text_words[a]\n",
    "print(len(a_hits_text))\n",
    "list(sorted(a_hits_text.items(),key=lambda x:x[1],reverse=True))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/bible_hits.json','w+') as file: \n",
    "    json.dump({'marginal':m_hits, 'in-text':t_hits},file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/author_hits.json','w+') as file: \n",
    "    json.dump({'marginal':a_hits_margin, 'in-text':a_hits_text},file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the most similar words by edit distance to the canonical and biblical hits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "from Levenshtein import distance \n",
    "def dist_fn(s1, s2):\n",
    "    return distance(s1, s2)\n",
    "\n",
    "def find_similar_words(target_word, word_list, threshold,k=20):\n",
    "    similar_words = process.extract(target_word, word_list, limit=None,scorer=dist_fn)\n",
    "    similar_words = [(word, dist) for word, dist in similar_words if dist <= threshold][-k:]\n",
    "    similar_words = sorted(similar_words, key=lambda x:x[1])\n",
    "    return similar_words\n",
    "\n",
    "def find_match(target_word, word_list):\n",
    "    match = process.extract(target_word, word_list, limit=None,scorer=dist_fn)\n",
    "    match = [(word, dist) for word, dist in match if dist == 0]\n",
    "    if len(match) > 0: \n",
    "        return True \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/author_hits.json','r') as file: \n",
    "    a_hits = json.load(file)\n",
    "\n",
    "with open('../assets/bible_hits.json','r') as file: \n",
    "    b_hits = json.load(file)\n",
    "\n",
    "a_hits = a_hits['marginal']\n",
    "b_hits = b_hits['marginal']\n",
    "len(a_hits),len(b_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_spelling(hits):  \n",
    "    for target in hits: \n",
    "        similar_words = find_similar_words(target, text_words.keys(),len(target)/2,10)\n",
    "        print(f\"Words similar to '{target}':\")\n",
    "        for word, dist in similar_words:\n",
    "            print(f\"{word} (Distance: {dist})\")\n",
    "        print()\n",
    "similar_spelling(b_hits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
