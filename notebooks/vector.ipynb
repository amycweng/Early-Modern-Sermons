{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from pprint import pprint\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.create_collection(\n",
    "      name=\"KJV_Bible\",\n",
    "      metadata={\"hnsw:space\": \"cosine\"}\n",
    "  )\n",
    "\n",
    "collection = client.get_collection(\n",
    "      name=\"KJV_Bible\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\",\n",
    "    \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\",\n",
    "    \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\",\n",
    "    \"they\", \"this\", \"to\", \"was\", \"will\", \"with\", \"i\", \"said\",\n",
    "    \"should\", \"from\", \"he\", \"have\", \"us\", \"our\", \"his\", \"shall\",\n",
    "    \"him\", \"so\", \"yet\",\"&\",\"^\",\"etc\",\"&c\",\"*\"\n",
    "]\n",
    "stopwords = {k:'' for k in stopwords}\n",
    "stopwords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,sys,re \n",
    "sys.path.append('../')\n",
    "from lib.standardization import * \n",
    "with open('../assets/encoded/A41135.json','r') as file:\n",
    "    data = json.load(file)\n",
    "encodings, info = data\n",
    "print(len(info),\"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = []\n",
    "for idx,e in enumerate(encodings):\n",
    "    current = [] \n",
    "    parts = e[3]\n",
    "    if e[2] == True: \n",
    "      continue # is a marginal note \n",
    "    for p in parts: \n",
    "        lemma = p[2]\n",
    "        pos = p[1]\n",
    "        if \"|\" in lemma: lemma = lemma.split(\"|\")[0]\n",
    "        if lemma == pos: continue\n",
    "        lemma = lemma.strip(\".\")\n",
    "        if lemma not in stopwords: \n",
    "            current.append(lemma)\n",
    "    tokenized_sentences.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/kjv-adorned.txt','r') as file:\n",
    "    kjv_tokens = file.readlines()\n",
    "\n",
    "bible = {}\n",
    "current_ver = None\n",
    "for t in kjv_tokens:\n",
    "    t = t.split(\"\\t\")\n",
    "    token, pos, lemma = t[0], t[2], t[4]\n",
    "    if token[0].isupper() and re.search(\"vv\",pos):\n",
    "        lemma = token\n",
    "        pos = \"np\"\n",
    "    if re.search(r'VERSE-',token):\n",
    "        current_ver = re.sub(\"VERSE-\", \"\",token)\n",
    "        bible[current_ver] = [[],[],[]]\n",
    "    elif token == pos: # punctuation mark\n",
    "        continue\n",
    "    else:\n",
    "        if lemma not in stopwords:\n",
    "            bible[current_ver][0].append(token)\n",
    "            bible[current_ver][1].append(pos)\n",
    "            bible[current_ver][2].append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = \"Matthew\"\n",
    "bible_labels = [label for label in bible if book in label]\n",
    "bible_verses = [\" \".join(bible[label][2]) for label in bible_labels]\n",
    "print(len(bible_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = \"Psalms\"\n",
    "bible_labels = [label for label in bible if book in label]\n",
    "bible_verses = [\" \".join(bible[label][2]) for label in bible_labels]\n",
    "print(len(bible_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tokenized_sentences: \n",
    "    if \"utter\" in sent and \"darkness\" in sent: \n",
    "       print(tokenized_sentences.index(sent),sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, label in enumerate(bible_labels): \n",
    "    print(label)\n",
    "    verse = bible_verses[idx]\n",
    "    collection.upsert(\n",
    "        documents=[verse],\n",
    "        ids=[label]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 1-5 grams and full sentence embedding\n",
    "sample = tokenized_sentences[328]\n",
    "all_phrases = [\" \".join(sample)]\n",
    "for i in range(1,6):\n",
    "  all_phrases.extend([\" \".join(item) for item in list(ngrams(sample, i))])\n",
    "print(all_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "for phrase in all_phrases: \n",
    "    results = collection.query(\n",
    "        query_texts=[phrase],\n",
    "        n_results=1,\n",
    "    )\n",
    "    all_results[phrase] = (results[\"ids\"][0][0],results[\"distances\"][0][0],results[\"documents\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = min([x[1] for x in all_results.values()])\n",
    "print(min_dist)\n",
    "min_dist = round(min_dist,1)\n",
    "print(min_dist)\n",
    "for phrase, items in all_results.items(): \n",
    "    if round(items[1],1) == min_dist: \n",
    "        print(phrase, items[0], items[1],items[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "        query_texts=[\"the fire kindle\"],\n",
    "        n_results=10,\n",
    "    )\n",
    "for i, distance in enumerate(results['distances'][0]): \n",
    "    print(results[\"distances\"][0][i])\n",
    "    pprint(results[\"documents\"][0][i])\n",
    "    print(results['ids'][0][i],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 1-5 grams and full sentence embedding\n",
    "sample = tokenized_sentences[1565]\n",
    "all_phrases = [\" \".join(sample)]\n",
    "for i in range(3,10):\n",
    "  all_phrases.extend([\" \".join(item) for item in list(ngrams(sample, i))])\n",
    "all_results = {}\n",
    "for phrase in all_phrases: \n",
    "    results = collection.query(\n",
    "        query_texts=[phrase],\n",
    "        n_results=1,\n",
    "    )\n",
    "    all_results[phrase] = (results[\"ids\"][0][0],results[\"distances\"][0][0],results[\"documents\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = min([x[1] for x in all_results.values()])\n",
    "min_dist = round(min_dist,1)\n",
    "for phrase, items in all_results.items(): \n",
    "    print(phrase, items[0], items[1],items[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "        query_texts=[\"utter darkness where be weep and gnash of\"],\n",
    "        n_results=10,\n",
    "    )\n",
    "for i, distance in enumerate(results['distances'][0]): \n",
    "    print(results[\"distances\"][0][i])\n",
    "    pprint(results[\"documents\"][0][i])\n",
    "    print(results['ids'][0][i],'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
