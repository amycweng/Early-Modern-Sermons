{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"/Users/amycweng/DH/db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the texts \n",
    "eras = [\"pre-Elizabethan\",\"Elizabethan\",\"Jacobean\",\"Carolinian\",\"CivilWar\",\"Interregnum\"]\n",
    "extra_eras = [\"Elizabethan2\", \"Jacobean2\", \"Carolinian2\",\"Interregnum2\"]\n",
    "eras.extend(extra_eras)\n",
    "collections = {}\n",
    "for era in eras:\n",
    "    # collections[era] = client.create_collection(name=era,metadata={\"hnsw:space\": \"cosine\"})\n",
    "    collections[era] = client.get_collection(name=era)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the marginalia  \n",
    "# pre1660marginalia = client.create_collection(name=\"pre1660marginalia\",metadata={\"hnsw:space\": \"cosine\"})\n",
    "pre1660marginalia = client.get_collection(name=\"pre1660marginalia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for era, collection in collections.items(): \n",
    "    print(era, len(collection.get()['ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../assets/pre1660.json') as file:\n",
    "    pre1660 = json.load(file)\n",
    "preE,E,J,C,CW,IR = pre1660\n",
    "eras = {\"pre-Elizabethan\":preE,\"Elizabethan\":E, \"Jacobean\":J, \"Carolinian\":C,\"CivilWar\":CW,\"Interregnum\":IR}\n",
    "tcpID_era = {}\n",
    "for era, era_dict in eras.items():\n",
    "    for id_list in era_dict.values():\n",
    "        for tcpID in id_list:\n",
    "            tcpID_era[tcpID] = era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    to_segment = [\", but\", \", while\", \", let\", \", they\", \", NONLATINALPHABET\",\n",
    "                    \", then\", \", yet\", \", than\", ', and yet', ', and though',\n",
    "                    ', at least', ', and to', ', this be', ', for', ', therefore',\n",
    "                    ', that', ', and we', ', and i ', ', when', ', and say', ', and this',\n",
    "                    ', and then', ', and than', ', and they', ', i say', ', as the apostle',\n",
    "                    ', otherwise', ', how', ', according', ', accordi^^', ', say',', and when',\n",
    "                    ', and he', ', and she', ', he say', ', she say', ', lest', ', and where',\n",
    "                    ', and how', ', and what', ', and there', ', and therefore', ', and thus',\n",
    "                    ', and if', ', and because', ', and I ', ', he will', ', they will', ', she will']\n",
    "    pattern = '|'.join(map(re.escape, to_segment))\n",
    "    parts = re.split(pattern, sentence)\n",
    "\n",
    "    matches = re.findall(pattern,sentence)\n",
    "    if len(parts) == 1: return parts\n",
    "    for idx, part in enumerate(parts):\n",
    "        if idx == (len(parts) - 1): break\n",
    "        conj = re.sub(\", \", \"\",matches[idx])\n",
    "        parts[idx] = part + \" , \"\n",
    "        parts[idx + 1] = conj + parts[idx+1]\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sermons():\n",
    "    def __init__(self,prefix):\n",
    "      self.prefix = prefix\n",
    "\n",
    "def get_docs(prefix):\n",
    "  corpus = Sermons(prefix)\n",
    "  with open(f'../assets/processed/{prefix}.json','r') as file:\n",
    "      sent_id, lemmatized, chunks, fw_subchunks = json.load(file)\n",
    "  corpus.sent_id = sent_id\n",
    "  corpus.lemmatized = lemmatized\n",
    "  corpus.sent_id_to_idx = {(tuple(x[0]),x[1]):idx for idx, x in enumerate(sent_id)}\n",
    "  passages = []\n",
    "  for id in corpus.sent_id:\n",
    "      if prefix in id[0][0]:\n",
    "          passage = corpus.lemmatized[corpus.sent_id_to_idx[(tuple(id[0]),id[1])]]\n",
    "          passage = re.sub(r\"[^A-Za-z\\^\\*,]\",\" \",passage)\n",
    "          passage = re.sub(r\"\\s+\",\" \", passage).strip(\" \")\n",
    "          passage = passage.strip(\" \")\n",
    "          if len(passage.split(\" \")) < 2: continue\n",
    "          parts = split_sentence(passage)\n",
    "          for part in parts:\n",
    "              if len(part.split(\" \")) < 3: continue\n",
    "              passages.append(part)\n",
    "  print(\"Passages:\", len(passages))\n",
    "  return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collections['pre-Elizabethan'].get(ids=[\"A0_217167\"])\n",
    "collections['CivilWar'].get(where={\"tcpID\":\"A67876\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(prefix,docs):\n",
    "    vectors = torch.load(f\"/Users/amycweng/DH/embeddings/{prefix}_corpus_embeddings_segmented.pth\",map_location=\"cpu\")\n",
    "    with open(f'/Users/amycweng/DH/embeddings/{prefix}_ids.json') as file:\n",
    "      ids = json.load(file)\n",
    "    count = 0\n",
    "    e, m, i,d = [],[],[],[]\n",
    "    prev_tcpID = None\n",
    "    s_count = 0\n",
    "    for idx, label in enumerate(ids):\n",
    "        tcpID, chunk_id, is_note = label[0]\n",
    "        part_id = label[1]\n",
    "\n",
    "        # check if the book is dated before 1660\n",
    "        if tcpID not in tcpID_era: continue\n",
    "        if int(tcpID[1:]) <= 67876: \n",
    "            count += 1 \n",
    "            continue\n",
    "\n",
    "        s_count += 1 \n",
    "        if prev_tcpID is None:\n",
    "            prev_tcpID = tcpID \n",
    "\n",
    "        if (tcpID != prev_tcpID): \n",
    "            if prev_tcpID not in processed:\n",
    "                collection = collections[tcpID_era[prev_tcpID]] \n",
    "                collection.upsert(ids=i,embeddings=e,metadatas=m,documents=d)\n",
    "                print('Processed',prev_tcpID,tcpID_era[prev_tcpID])\n",
    "                processed[prev_tcpID] = True \n",
    "            prev_tcpID = tcpID\n",
    "            e, m, i, d = [],[],[],[]\n",
    "            s_count = 0\n",
    "        elif (s_count > 0) and ((s_count % 20000) == 0):\n",
    "            if len(e) > 0:\n",
    "                collection = collections[tcpID_era[prev_tcpID]]\n",
    "                collection.upsert(ids=i,embeddings=e,metadatas=m,documents=d)\n",
    "                e, m, i, d = [],[],[],[]\n",
    "                print('Processed part of',prev_tcpID,tcpID_era[prev_tcpID])\n",
    "\n",
    "        count += 1\n",
    "        e.append(vectors[idx].tolist())\n",
    "        m.append({\"tcpID\": tcpID, 'chunk_id': chunk_id, 'is_note':is_note, 'part_id':part_id})\n",
    "        i.append(f'{prefix}_{idx}')\n",
    "        d.append(docs[idx])\n",
    "\n",
    "    if len(i) > 0 and prev_tcpID not in processed:\n",
    "        collection = collections[tcpID_era[prev_tcpID]]\n",
    "        collection.add(ids=i,embeddings=e,metadatas=m,documents=d)\n",
    "        print('Processed',prev_tcpID,tcpID_era[prev_tcpID])\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"A9\" \n",
    "data = get_docs(prefix)\n",
    "process(prefix,data)\n",
    "del data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "items =  collections['Elizabethan'].get(include=[\"metadatas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = []\n",
    "for item in items[\"metadatas\"]: \n",
    "    prefixes.append(item[\"tcpID\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A1': 431384,\n",
       "         'A0': 173710,\n",
       "         'B1': 27034,\n",
       "         'A2': 10803,\n",
       "         'B0': 10554,\n",
       "         'A6': 10250})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "Counter(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections[\"Elizabethan_A1\"] = client.create_collection(name=\"Elizabethan_A1\",metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(items[\"metadatas\"]): \n",
    "    if item[\"tcpID\"][:2] == \"A1\": \n",
    "        embedding = collections[\"Elizabethan\"].get(ids=[items[\"ids\"][idx]])\n",
    "        collections[\"Elizabethan_A1\"].add"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
