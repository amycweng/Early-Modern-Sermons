{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "class Sermons():\n",
    "    def __init__(self, prefix):\n",
    "        self.prefix = prefix\n",
    "        self.sent_id = [] # tuples of ((tcpID, chunk idx, location), subchunk idx) representing the subchunk's ID\n",
    "        self.sentences = [] # subchunk strings that are lemmatized\n",
    "        self.fw_subchunks = {} # IDs of subchunks with more than three foreign words\n",
    "        self.chunks = [] # subchunk strings that are tokenized\n",
    "\n",
    "        self.get_texts_from_json()\n",
    "        self.get_marginalia_from_json()\n",
    "        self.get_indices()\n",
    "        self.get_chunks(self.chunks)\n",
    "\n",
    "    def get_indices(self):\n",
    "        self.sent_id_to_idx = {x:idx for idx, x in enumerate(self.sent_id)}\n",
    "\n",
    "    def get_marginalia_from_json(self):\n",
    "        # reorganize marginalia dictionary from the JSON file\n",
    "        with open(f'../assets/processed/{self.prefix}_marginalia.json','r') as file:\n",
    "            marginalia = json.load(file)\n",
    "            print('Loaded marginalia')\n",
    "        new_marginalia = {}\n",
    "        note_id = 0 # id of the note within the current sentence chunk\n",
    "        curr_s = 0 # index of the current sentence chunk\n",
    "        for tcpID, items in marginalia.items():\n",
    "            if tcpID not in new_marginalia:\n",
    "                new_marginalia[tcpID] = {}\n",
    "                note_id = 0\n",
    "            for item in items:\n",
    "              s_idx = item[0]\n",
    "              if (s_idx,note_id) not in new_marginalia[tcpID]:\n",
    "                  if s_idx != curr_s:\n",
    "                      note_id = 0\n",
    "                  else:\n",
    "                      note_id += 1\n",
    "                  new_marginalia[tcpID][(s_idx, note_id)] = []\n",
    "              else:\n",
    "                  note_id += 1\n",
    "\n",
    "              if s_idx != curr_s: curr_s = s_idx\n",
    "              new_marginalia[tcpID][(s_idx, note_id)] = item[-1]\n",
    "        marginalia = new_marginalia\n",
    "        self.create_corpus(marginalia,True)\n",
    "        print('Processed marginalia')\n",
    "\n",
    "    def get_texts_from_json(self):\n",
    "        with open(f'../assets/processed/{self.prefix}_texts.json','r') as file:\n",
    "            texts = json.load(file)\n",
    "            print('Loaded texts')\n",
    "        self.create_corpus(texts)\n",
    "        print('Processed texts')\n",
    "\n",
    "    def create_corpus(self, data,is_margin=False):\n",
    "      for tcpID, items in data.items():\n",
    "          if self.prefix not in tcpID: continue\n",
    "\n",
    "          for s_idx, encodings in items.items():\n",
    "              current = []\n",
    "              tokens = []\n",
    "              part_id = 0\n",
    "\n",
    "              if is_margin:\n",
    "                  s_idx, note_id = s_idx\n",
    "                  s_idx = int(s_idx)\n",
    "                  sid = (tcpID,s_idx,note_id)\n",
    "              else:\n",
    "                  s_idx = int(s_idx)\n",
    "                  sid = (tcpID,s_idx,-1)\n",
    "\n",
    "              fw = [] # contains indices of the foreign words\n",
    "              fw_idx = 0\n",
    "\n",
    "              for idx, x in enumerate(encodings):\n",
    "                  segment = False\n",
    "\n",
    "                  token, pos, lemma = x[0], x[1], x[2]\n",
    "                  if len(token) == 0: continue\n",
    "                  tokens.append(token)\n",
    "                  if lemma == \"encage\" and token.isdigit():\n",
    "                      current.append(x[0])\n",
    "                  else:\n",
    "                      current.append(lemma)\n",
    "\n",
    "                  if 'fw' in pos: # foreign words\n",
    "                      fw.append(fw_idx)\n",
    "                  fw_idx +=1\n",
    "\n",
    "                  if \".\" in pos or ';' in pos or ':' in pos or '?' in pos:\n",
    "                      segment = True\n",
    "                  elif \".\" in token:\n",
    "                      if (idx+1) < len(encodings):\n",
    "                        if len(encodings[idx+1][0]) == 0: continue\n",
    "                        if encodings[idx+1][0][0].isupper(): # case of \"L. 6 17. By faith Noah\"\n",
    "                            segment = True\n",
    "\n",
    "                  def check_foreign(fw):\n",
    "                      consec = 1\n",
    "                      for i in range(len(fw) - 1):\n",
    "                          if fw[i] + 1 == fw[i + 1]:\n",
    "                              consec += 1\n",
    "                              if consec == 3:\n",
    "                                  return True\n",
    "                          else:\n",
    "                              consec = 1\n",
    "                      return False\n",
    "\n",
    "                  if segment or (idx == (len(encodings)-1)):\n",
    "                      self.sentences.append(\" \".join(current))\n",
    "                      self.sent_id.append((sid,part_id))\n",
    "                      self.chunks.append(\" \".join(tokens))\n",
    "                      if check_foreign(fw):\n",
    "                          self.fw_subchunks[f\"{sid[0]},{sid[1]},{sid[2]},{part_id}\"] = fw\n",
    "\n",
    "                      current = []\n",
    "                      tokens = []\n",
    "                      fw = []\n",
    "\n",
    "                      fw_idx = 0\n",
    "                      part_id += 1\n",
    "    \n",
    "    def get_chunks(self, original):\n",
    "      chunks = {} # keys are just (tcpID, chunk idx, is_note)\n",
    "      for s_id, part_id in self.sent_id:\n",
    "          if s_id[-1] > -1:\n",
    "              curr_sid = f\"{s_id[0]},{s_id[1]},True\"\n",
    "              if curr_sid not in chunks:\n",
    "                  chunks[curr_sid] = {}\n",
    "              if s_id[-1] not in chunks[curr_sid]:\n",
    "                  chunks[curr_sid][s_id[-1]] = original[self.sent_id_to_idx[(s_id,part_id)]]\n",
    "              else: \n",
    "                  chunks[curr_sid][s_id[-1]] = chunks[curr_sid][s_id[-1]] + \" \" + original[self.sent_id_to_idx[(s_id,part_id)]]\n",
    "          else:\n",
    "              curr_sid = f\"{s_id[0]},{s_id[1]},False\"\n",
    "              if curr_sid not in chunks:\n",
    "                  chunks[curr_sid] = original[self.sent_id_to_idx[(s_id,part_id)]]\n",
    "              else: \n",
    "                  chunks[curr_sid] = chunks[curr_sid] + \" \" + original[self.sent_id_to_idx[(s_id,part_id)]]\n",
    "      self.chunks = chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = ['B','A0','A1','A2','A3','A4','A5','A6','A7','A8','A9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in prefixes: \n",
    "    print(prefix)\n",
    "    corpus = Sermons(prefix)\n",
    "    with open(f'../assets/processed/{prefix}.json','w+') as file: \n",
    "        json.dump([corpus.sent_id,corpus.sentences,corpus.chunks,corpus.fw_subchunks],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "sermons_metadata = pd.read_csv(\"../assets/sermons.csv\")\n",
    "dates = sermons_metadata[\"date\"]\n",
    "subjects = sermons_metadata[\"subject_headings\"]\n",
    "all_subjects = {}\n",
    "titles = {}\n",
    "orig_authors = {}\n",
    "pubplace = {}\n",
    "tcpID_dates = {}\n",
    "for idx,s in enumerate(subjects): \n",
    "    tcpID = sermons_metadata['id'][idx]\n",
    "    titles[tcpID] = sermons_metadata['title'][idx]\n",
    "    orig_authors[tcpID] = sermons_metadata['authors'][idx]\n",
    "    pubplace[tcpID] = sermons_metadata['pubplace'][idx]\n",
    "    all_subjects[tcpID] = [_.strip('.') for _ in s.split(\"; \")]\n",
    "    date = sermons_metadata['date'][idx]\n",
    "    if \"-\" in date: \n",
    "        date = int(date.split(\"-\")[0])\n",
    "    tcpID_dates[tcpID] = int(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_era(start,end):\n",
    "    era = {k:[] for k in prefixes}\n",
    "    for tcpID, date in tcpID_dates.items(): \n",
    "        if 'B' in tcpID: \n",
    "            prefix = 'B'\n",
    "        else: \n",
    "            prefix = tcpID[:2]\n",
    "        if start <= date <= end:\n",
    "            era[prefix].append(tcpID)\n",
    "    return era "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "232\n",
      "465\n",
      "371\n",
      "461\n",
      "405\n"
     ]
    }
   ],
   "source": [
    "preElizabethan = get_era(1400,1557)\n",
    "print(sum([len(x) for x in preElizabethan.values()]))\n",
    "Elizabethan = get_era(1558,1602)\n",
    "print(sum([len(x) for x in Elizabethan.values()]))\n",
    "Jacobean = get_era(1603,1624)\n",
    "print(sum([len(x) for x in Jacobean.values()]))\n",
    "Carolinian = get_era(1625,1641)\n",
    "print(sum([len(x) for x in Carolinian.values()]))\n",
    "CivilWar = get_era(1642,1649)\n",
    "print(sum([len(x) for x in CivilWar.values()]))\n",
    "Interregnum = get_era(1650,1659)\n",
    "print(sum([len(x) for x in Interregnum.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../assets/pre1660.json','w+') as file: \n",
    "    json.dump([preElizabethan,Elizabethan,Jacobean,Carolinian,CivilWar,Interregnum],file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
