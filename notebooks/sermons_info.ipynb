{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,csv \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "class Sermons():\n",
    "    def __init__(self, era, prefix):\n",
    "        self.era = era\n",
    "        self.prefix = prefix\n",
    "        self.sent_id = [] # tuples of ((tcpID, chunk idx, location), subchunk idx) representing the subchunk's ID\n",
    "        self.lemmata = [] # subchunk strings that are lemmatized\n",
    "        self.fw_subchunks = {} # IDs of subchunks with more than three foreign words\n",
    "        self.tokens = [] # subchunk strings that are tokenized\n",
    "\n",
    "        self.get_texts_from_json()\n",
    "        self.get_marginalia_from_json()\n",
    "        self.get_indices()\n",
    "\n",
    "    def get_indices(self):\n",
    "        self.sent_id_to_idx = {x:idx for idx, x in enumerate(self.sent_id)}\n",
    "\n",
    "    def get_marginalia_from_json(self):\n",
    "        # reorganize marginalia dictionary from the JSON file\n",
    "        with open(f'../assets/processed/{self.era}/json/{self.prefix}_marginalia.json','r') as file:\n",
    "            marginalia = json.load(file)\n",
    "            print('Loaded marginalia')\n",
    "        new_marginalia = {}\n",
    "        note_id = 0 # id of the note within the current sentence chunk\n",
    "        curr_s = 0 # index of the current sentence chunk\n",
    "        for tcpID, items in marginalia.items():\n",
    "            if tcpID not in new_marginalia:\n",
    "                new_marginalia[tcpID] = {}\n",
    "                note_id = 0\n",
    "            for item in items:\n",
    "              s_idx = item[0]\n",
    "              if (s_idx,note_id) not in new_marginalia[tcpID]:\n",
    "                  if s_idx != curr_s:\n",
    "                      note_id = 0\n",
    "                  else:\n",
    "                      note_id += 1\n",
    "                  new_marginalia[tcpID][(s_idx, note_id)] = []\n",
    "              else:\n",
    "                  note_id += 1\n",
    "\n",
    "              if s_idx != curr_s: curr_s = s_idx\n",
    "              new_marginalia[tcpID][(s_idx, note_id)] = item[-1]\n",
    "        marginalia = new_marginalia\n",
    "        if len(marginalia) > 0: \n",
    "            self.create_corpus(marginalia,True)\n",
    "            print('Processed marginalia')\n",
    "\n",
    "    def get_texts_from_json(self):\n",
    "        with open(f'../assets/processed/{self.era}/json/{self.prefix}_texts.json','r') as file:\n",
    "            texts = json.load(file)\n",
    "            print('Loaded texts')\n",
    "        self.create_corpus(texts)\n",
    "        print('Processed texts')\n",
    "\n",
    "    def create_corpus(self, data,is_margin=False):\n",
    "      for tcpID, items in data.items():          \n",
    "          for s_idx, encodings in items.items():\n",
    "              current = []\n",
    "              tokens = []\n",
    "              part_id = 0\n",
    "\n",
    "              if is_margin:\n",
    "                  s_idx, note_id = s_idx\n",
    "                  s_idx = int(s_idx)\n",
    "                  sid = (tcpID,s_idx,note_id)\n",
    "              else:\n",
    "                  s_idx = int(s_idx)\n",
    "                  sid = (tcpID,s_idx,-1)\n",
    "\n",
    "              fw = [] # contains indices of the foreign words\n",
    "              fw_idx = 0\n",
    "\n",
    "              for idx, x in enumerate(encodings):\n",
    "                  segment = False\n",
    "\n",
    "                  token, pos, lemma = x[0], x[1], x[2]\n",
    "                  if len(token) == 0: continue\n",
    "                  tokens.append(token)\n",
    "                  if lemma == \"encage\" and token.isdigit():\n",
    "                      current.append(x[0])\n",
    "                  else:\n",
    "                      current.append(lemma)\n",
    "\n",
    "                  if 'fw' in pos: # foreign words\n",
    "                      fw.append(fw_idx)\n",
    "                  fw_idx +=1\n",
    "\n",
    "                  if \".\" in pos or ';' in pos or ':' in pos or '?' in pos:\n",
    "                      segment = True\n",
    "                  elif \".\" in token:\n",
    "                      if (idx+1) < len(encodings):\n",
    "                        if len(encodings[idx+1][0]) == 0: continue\n",
    "                        if encodings[idx+1][0][0].isupper(): # case of \"L. 6 17. By faith Noah\"\n",
    "                            segment = True\n",
    "\n",
    "                  def check_foreign(fw):\n",
    "                      consec = 1\n",
    "                      for i in range(len(fw) - 1):\n",
    "                          if fw[i] + 1 == fw[i + 1]:\n",
    "                              consec += 1\n",
    "                              if consec == 3:\n",
    "                                  return True\n",
    "                          else:\n",
    "                              consec = 1\n",
    "                      return False\n",
    "\n",
    "                  if segment or (idx == (len(encodings)-1)):\n",
    "                      self.lemmata.append(\" \".join(current))\n",
    "                      self.sent_id.append((sid,part_id))\n",
    "                      self.tokens.append(\" \".join(tokens))\n",
    "                      if check_foreign(fw):\n",
    "                          self.fw_subchunks[f\"{sid[0]},{sid[1]},{sid[2]},{part_id}\"] = fw\n",
    "\n",
    "                      current = []\n",
    "                      tokens = []\n",
    "                      fw = []\n",
    "\n",
    "                      fw_idx = 0\n",
    "                      part_id += 1\n",
    "    \n",
    "    def get_chunks(self, targets):\n",
    "      chunks = {} # keys are just (tcpID, chunk idx, is_note)\n",
    "      for s_id, part_id in self.sent_id:\n",
    "          if s_id[-1] > -1:\n",
    "              curr_sid = f\"{s_id[0]},{s_id[1]},True\"\n",
    "              if curr_sid not in chunks:\n",
    "                  chunks[curr_sid] = {}\n",
    "              if s_id[-1] not in chunks[curr_sid]:\n",
    "                  chunks[curr_sid][s_id[-1]] = targets[self.sent_id_to_idx[(s_id,part_id)]]\n",
    "              else: \n",
    "                  chunks[curr_sid][s_id[-1]] = chunks[curr_sid][s_id[-1]] + \" \" + targets[self.sent_id_to_idx[(s_id,part_id)]]\n",
    "          else:\n",
    "              curr_sid = f\"{s_id[0]},{s_id[1]},False\"\n",
    "              if curr_sid not in chunks:\n",
    "                  chunks[curr_sid] = targets[self.sent_id_to_idx[(s_id,part_id)]]\n",
    "              else: \n",
    "                  chunks[curr_sid] = chunks[curr_sid] + \" \" + targets[self.sent_id_to_idx[(s_id,part_id)]]\n",
    "      return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "Loaded texts\n",
      "Processed texts\n",
      "Loaded marginalia\n",
      "Processed marginalia\n",
      "B body done\n",
      "B marginalia done\n",
      "A0\n",
      "Loaded texts\n",
      "Processed texts\n",
      "Loaded marginalia\n",
      "Processed marginalia\n",
      "A0 body done\n",
      "A0 marginalia done\n",
      "A1\n",
      "Loaded texts\n",
      "Processed texts\n",
      "Loaded marginalia\n",
      "Processed marginalia\n",
      "A1 body done\n",
      "A1 marginalia done\n",
      "A2\n",
      "Loaded texts\n",
      "Processed texts\n",
      "Loaded marginalia\n",
      "Processed marginalia\n",
      "A2 body done\n",
      "A2 marginalia done\n",
      "A3\n",
      "A4\n",
      "A5\n",
      "A6\n",
      "Loaded texts\n",
      "Processed texts\n",
      "Loaded marginalia\n",
      "Processed marginalia\n",
      "A6 body done\n",
      "A6 marginalia done\n",
      "A7\n",
      "Loaded texts\n",
      "Processed texts\n",
      "Loaded marginalia\n",
      "Processed marginalia\n",
      "A7 body done\n",
      "A7 marginalia done\n",
      "A8\n",
      "A9\n"
     ]
    }
   ],
   "source": [
    "with open('../assets/corpora.json','r') as file: \n",
    "    corpora = json.load(file)\n",
    "\n",
    "era = \"pre-Elizabethan\"\n",
    "\n",
    "for prefix,tcpIDs in corpora[era].items():\n",
    "    print(prefix)\n",
    "    if len(tcpIDs) == 0: continue\n",
    "\n",
    "    tcpIDs = sorted(tcpIDs)\n",
    "    corpus = Sermons(\"pre-Elizabethan\",prefix)\n",
    "    body_formatted = []\n",
    "    margins_formatted = []\n",
    "\n",
    "    tokenized = corpus.get_chunks(corpus.tokens)\n",
    "    lemmatized = corpus.get_chunks(corpus.lemmata)\n",
    "\n",
    "    with open(f\"../assets/processed/{era}/json/{prefix}_info.json\") as file: \n",
    "        info = json.load(file)\n",
    "\n",
    "    for key, segment in tokenized.items():\n",
    "        key = key.split(\",\")\n",
    "        i = info[key[0]][key[1]]\n",
    "        if i[1] is None: loc_type = None\n",
    "        elif 'IMAGE' in i[1]: loc_type = \"IMAGE\"\n",
    "        else: loc_type = \"PAGE\"\n",
    "\n",
    "        if key[-1] == 'False':\n",
    "            body_formatted.append({\n",
    "                'tcpID': key[0],\n",
    "                'sid': key[1],\n",
    "                'section': i[0],\n",
    "                'loc': [i[1].split(loc_type)[-1] if loc_type is not None else None][0], \n",
    "                'loc_type': loc_type, \n",
    "                'pid': i[2], \n",
    "                'tokens': segment, \n",
    "                'lemmatized': lemmatized[\",\".join(key)]\n",
    "            })\n",
    "        else: \n",
    "            for nid, part in segment.items(): \n",
    "                margins_formatted.append({\n",
    "                    'tcpID': key[0],\n",
    "                    'sid': key[1],\n",
    "                    'nid': nid,\n",
    "                    'tokens': segment[nid], \n",
    "                    'lemmatized': lemmatized[\",\".join(key)][nid]\n",
    "                })\n",
    "    \n",
    "    if len(body_formatted) == 0: continue\n",
    "\n",
    "    with open(f'/Users/amycweng/DH/SERMONS_APP/db/data/{era}/{prefix}_body.csv','w+') as file: \n",
    "        writer = csv.DictWriter(file, fieldnames=body_formatted[0].keys())\n",
    "        writer.writerows(body_formatted)\n",
    "        print(f'{prefix} body done')\n",
    "    with open(f'/Users/amycweng/DH/SERMONS_APP/db/data/{era}/{prefix}_margin.csv','w+') as file: \n",
    "        writer = csv.DictWriter(file, fieldnames=margins_formatted[0].keys())\n",
    "        writer.writerows(margins_formatted)\n",
    "        print(f'{prefix} marginalia done')\n",
    "\n",
    "    with open(f'../assets/processed/{era}/sub-segments/{prefix}.json','w+') as file: \n",
    "        json.dump([corpus.sent_id,corpus.lemmata,corpus.fw_subchunks],file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
