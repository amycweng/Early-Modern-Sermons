{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for information retrieval fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Bible verse to body segments with the relevant citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,json,os\n",
    "import pandas as pd \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(era,c_type=\"verse\"): \n",
    "    relevant = {}\n",
    "    with open(f\"../assets/citations/{era}_citation_segments.json\") as file:\n",
    "        c_to_seg = json.load(file)\n",
    "    seg_to_c = {}\n",
    "    for cited, segments in c_to_seg.items():\n",
    "        if \"Ibidem\" in cited: continue \n",
    "        cited = \" \".join(cited.split(\"-\"))\n",
    "        if re.search(r\"\\d+ \\d+\",cited):\n",
    "            cited = cited.split(\" \")\n",
    "            if c_type == \"verse\": \n",
    "                cited = \" \".join(cited[:-2]) +\" \" + \".\".join(cited[-2:])\n",
    "            else: cited = \" \".join(cited)\n",
    "        for s in segments: \n",
    "            seg_id = (s.split(\",\")[0],int(s.split(\",\")[1]))\n",
    "            if seg_id not in seg_to_c: \n",
    "                seg_to_c[seg_id] = []\n",
    "            seg_to_c[seg_id].append(cited)\n",
    "    print(era, len(c_to_seg),'citations',len(seg_to_c),\"segments\")\n",
    "\n",
    "    for fp in tqdm(os.listdir(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}\")):\n",
    "        if \"body\" not in fp: continue \n",
    "        text = pd.read_csv(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}/{fp}\", header=None)\n",
    "        for idx, tcpID in enumerate(text[0]):\n",
    "            sidx = str(text[1][idx])\n",
    "            \n",
    "            close_to_citation = False \n",
    "            citation_sidx = []\n",
    "            # window of 2 segments \n",
    "            if (tcpID,int(sidx)) in seg_to_c:\n",
    "                close_to_citation = True\n",
    "                citation_sidx.append(int(sidx))\n",
    "            else: \n",
    "                i = 1 \n",
    "                if (tcpID,int(sidx)-i) in seg_to_c:\n",
    "                    close_to_citation = True\n",
    "                    citation_sidx.append(int(sidx)-i)\n",
    "                elif (tcpID,int(sidx)+i) in seg_to_c:\n",
    "                    close_to_citation = True \n",
    "                    citation_sidx.append(int(sidx)+i)\n",
    "            if close_to_citation: \n",
    "                s = text[7][idx]\n",
    "                t = text[6][idx]\n",
    "                t = re.sub(r\"\\<\\/i\\>|\\<NOTE\\>|NONLATINALPHABET|\\<i\\>\",\"\",t)\n",
    "                t = re.sub(r\"\\s+\",\" \",t)\n",
    "                t = t.strip(\" \")\n",
    "                if len(t.split(\" \"))< 5: continue # at least 5 words long\n",
    "                if s not in relevant: \n",
    "                    relevant[s] = ({},{},{}) # citations, original, location \n",
    "                relevant[s][2][(tcpID,sidx)] = None \n",
    "                relevant[s][1][t] = None \n",
    "                for entry in citation_sidx: \n",
    "                    for c in seg_to_c[(tcpID,entry)]:\n",
    "                        relevant[s][0][c] = None \n",
    "    for s,r in relevant.items(): \n",
    "        relevant[s] = (list(r[0].keys()), list(r[1].keys()), list(r[2].keys()))\n",
    "    print(len(set(relevant)),'unique passages')\n",
    "    with open(f\"../assets/relevant/{era}_citations.json\",\"w+\") as file: \n",
    "        json.dump(relevant, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-Elizabethan 1255 citations 5335 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13180 unique passages\n",
      "Elizabethan 18612 citations 66304 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:18<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157078 unique passages\n",
      "Jacobean 28028 citations 128122 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:23<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282511 unique passages\n",
      "Carolinian 29558 citations 125648 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:23<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279490 unique passages\n",
      "CivilWar 24626 citations 76868 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182425 unique passages\n",
      "Interregnum 29512 citations 137974 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 23/28 [00:19<00:05,  1.02s/it]/var/folders/3_/ylrg8wdj20l755p4921q0wg80000gp/T/ipykernel_49079/1534402195.py:23: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  text = pd.read_csv(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}/{fp}\", header=None)\n",
      "100%|██████████| 28/28 [00:28<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313682 unique passages\n",
      "CharlesII 36222 citations 260707 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:59<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600644 unique passages\n",
      "JamesII 11290 citations 17852 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:04<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42781 unique passages\n",
      "WilliamAndMary 24357 citations 160023 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:24<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333484 unique passages\n"
     ]
    }
   ],
   "source": [
    "with open('../assets/corpora.json',\"r\") as file: \n",
    "    eras = json.load(file)\n",
    "for era in eras: \n",
    "    process(era)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get non-citation segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218958"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"../assets\"\n",
    "locations = {}\n",
    "for fp in os.listdir(f\"{folder}/relevant\"):\n",
    "    if not re.search(f'CivilWar',fp): continue\n",
    "    else:\n",
    "      with open(f\"{folder}/relevant/{fp}\",\"r\") as file:\n",
    "        r = json.load(file)\n",
    "        for k, v in r.items(): \n",
    "            loc = v[2]\n",
    "            for l in loc: \n",
    "               locations[tuple(l)] = None \n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Body Texts  \n",
    "from tqdm import tqdm \n",
    "\n",
    "def get_era(era):\n",
    "    parts = {}\n",
    "    for fp in tqdm(os.listdir(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}\")):\n",
    "        if \"body\" not in fp: continue \n",
    "        text = pd.read_csv(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}/{fp}\", header=None)\n",
    "        text = text.to_dict(orient=\"records\")\n",
    "        for item in text:\n",
    "            original = item[6]\n",
    "            original = re.sub(r\"\\<\\/i\\>|\\<NOTE\\>|NONLATINALPHABET|\\<i\\>\",\"\",original)\n",
    "            original = re.sub(r\"\\s+\",\" \",original)\n",
    "            original = original.strip(\" \")\n",
    "            tcpID, sidx = item[0], item[1]\n",
    "            s = item[7]\n",
    "            if not isinstance(s,str): continue \n",
    "            if len(s) == 0: continue \n",
    "            if s not in parts: \n",
    "                parts[s] = ({},{}) # original, locations \n",
    "            parts[s][1][(tcpID,str(sidx))] = None \n",
    "            parts[s][0][original] = None \n",
    "    for s,r in parts.items(): \n",
    "        parts[s] = [[str(k) for k in r[0]], list(r[1].keys())]\n",
    "\n",
    "    batches = []\n",
    "    batch_size = 40000\n",
    "    batch_num = 0 \n",
    "    progress = tqdm( range(0, len(parts), batch_size))\n",
    "    for i in progress:\n",
    "        batch = {}\n",
    "        for p in list(parts.keys())[i: i + batch_size]:\n",
    "            batch[p] = parts[p]\n",
    "        batches.append(batch)\n",
    "        with open(f\"../assets/unique_body/{era}_{batch_num}.json\",\"w+\") as file: \n",
    "            json.dump(parts,file)\n",
    "        batch_num += 1 \n",
    "\n",
    "    print(f\"{era}: Total {len(parts)} parts.\")\n",
    "\n",
    "    # Marginalia \n",
    "    parts = {}\n",
    "    for fp in tqdm(os.listdir(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}\")):\n",
    "        if \"margin\" not in fp: continue \n",
    "        margin = pd.read_csv(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}/{fp}\", header=None, names=[\"tcpID\",\"sidx\",\"nidx\",\"original\",\"standardized\"])\n",
    "        margin = margin.to_dict(orient=\"records\")\n",
    "        for m in margin:\n",
    "            t = m[\"original\"]\n",
    "            original = re.sub(r\"\\<\\/i\\>|\\<NOTE\\>|NONLATINALPHABET|\\<i\\>\",\"\",t)\n",
    "            original = re.sub(r\"\\s+\",\" \",original)\n",
    "            t = original.strip(\" \")\n",
    "            tcpID, sidx, nidx = m[\"tcpID\"], m[\"sidx\"], m[\"nidx\"]\n",
    "            s = m[\"standardized\"]\n",
    "            if not isinstance(s,str): continue \n",
    "            if len(s) == 0: continue \n",
    "            if s not in parts: \n",
    "                parts[s] = ({},{}) # original, locations \n",
    "            parts[s][1][(tcpID,str(sidx),str(nidx))] = None \n",
    "            parts[s][0][t] = None \n",
    "    for s,r in parts.items(): \n",
    "        parts[s] = [[str(k) for k in r[0]], list(r[1].keys())]\n",
    "    with open(f\"../assets/unique_body/{era}_margin.json\",\"w+\") as file: \n",
    "        json.dump(parts,file)\n",
    "    print(f\"{era} marginalia: Total {len(parts)} parts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  6.49it/s]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-Elizabethan: Total 109838 parts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 33.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-Elizabethan marginalia: Total 7066 parts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_era(\"pre-Elizabethan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_era(\"Elizabethan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_era(\"Jacobean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_era(\"Carolinian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_era(\"Interregnum\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
