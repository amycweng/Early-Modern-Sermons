{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for information retrieval fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Bible verse to body segments with the relevant citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,json,os\n",
    "import pandas as pd \n",
    "def combine_punc_with_text(segment):\n",
    "    segment = re.sub(r'\\s+([,.?!;:)])', r'\\1', segment)\n",
    "    segment = re.sub(r'([(])\\s+', r'\\1', segment)\n",
    "    segment = re.sub(r\"\\s+\",\" \",segment)\n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(era,c_type=\"verse\"): \n",
    "    relevant = {}\n",
    "    with open(f\"../assets/citations/{era}_{c_type}_citation_segments.json\") as file:\n",
    "        c_to_seg = json.load(file)\n",
    "    seg_to_c = {}\n",
    "    for cited, segments in c_to_seg.items():\n",
    "        if \"Ibidem\" in cited: continue \n",
    "        cited = \" \".join(cited.split(\"-\"))\n",
    "        if re.search(r\"\\d+ \\d+\",cited):\n",
    "            cited = cited.split(\" \")\n",
    "            if c_type == \"verse\": \n",
    "                cited = \" \".join(cited[:-2]) +\" \" + \".\".join(cited[-2:])\n",
    "            else: cited = \" \".join(cited)\n",
    "        for s in segments: \n",
    "            seg_id = (s.split(\",\")[0],int(s.split(\",\")[1]))\n",
    "            if seg_id not in seg_to_c: \n",
    "                seg_to_c[seg_id] = []\n",
    "            seg_to_c[seg_id].append(cited)\n",
    "    print(era, len(c_to_seg),'citations',len(seg_to_c),\"segments\")\n",
    "\n",
    "    for prefix in os.listdir(f\"../assets/processed/{era}/sub-segments\"):\n",
    "        with open(f\"../assets/processed/{era}/sub-segments/{prefix}\",'r') as file:\n",
    "            s_ids, s_text, s_orig, _ = json.load(file)\n",
    "        \n",
    "        for idx, s_id in enumerate(s_ids):\n",
    "            tcpID, sidx, nidx = s_id[0][0], str(s_id[0][1]), s_id[0][2]\n",
    "            if nidx >= 0: continue # only body content\n",
    "            if (tcpID,int(sidx)) in seg_to_c: \n",
    "                s = combine_punc_with_text(s_text[idx])\n",
    "                t = combine_punc_with_text(s_orig[idx])\n",
    "                if len(t.split(\" \"))< 5: continue # at least 5 words long\n",
    "                if s not in relevant: \n",
    "                    relevant[s] = ({},{},{}) # citations, original, location \n",
    "                relevant[s][2][(tcpID,sidx,str(s_id[1]))] = None \n",
    "                relevant[s][1][t] = None \n",
    "                for c in seg_to_c[(tcpID,int(sidx))]:\n",
    "                    relevant[s][0][c] = None \n",
    "    for s,r in relevant.items(): \n",
    "        relevant[s] = (list(r[0].keys()), list(r[1].keys()), list(r[2].keys()))\n",
    "    print(len(set(relevant)),'unique passages')\n",
    "    if c_type == \"chapter\": \n",
    "        with open(f\"../assets/relevant/{era}_chapter_citations.json\",\"w+\") as file: \n",
    "            json.dump(relevant, file)\n",
    "    else: \n",
    "        with open(f\"../assets/relevant/{era}.json\",\"w+\") as file: \n",
    "            json.dump(relevant, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elizabethan 15541 citations 26648 segments\n",
      "80586 unique passages\n",
      "Carolinian 25188 citations 68937 segments\n",
      "259178 unique passages\n",
      "WilliamAndMary 20994 citations 65281 segments\n",
      "231206 unique passages\n",
      "pre-Elizabethan 434 citations 476 segments\n",
      "970 unique passages\n",
      "Jacobean 24121 citations 71260 segments\n",
      "247392 unique passages\n",
      "CharlesII 32891 citations 169548 segments\n",
      "644862 unique passages\n",
      "CivilWar 22412 citations 45459 segments\n",
      "190585 unique passages\n",
      "JamesII 9997 citations 11107 segments\n",
      "42944 unique passages\n",
      "Interregnum 25695 citations 84429 segments\n",
      "350522 unique passages\n"
     ]
    }
   ],
   "source": [
    "for era in os.listdir('../assets/processed'): \n",
    "    if era == \".DS_Store\": continue \n",
    "    process(era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elizabethan 1988 citations 22602 segments\n",
      "60760 unique passages\n",
      "Carolinian 2177 citations 20759 segments\n",
      "91236 unique passages\n",
      "WilliamAndMary 1664 citations 9991 segments\n",
      "39221 unique passages\n",
      "pre-Elizabethan 910 citations 4143 segments\n",
      "8389 unique passages\n",
      "Jacobean 2229 citations 23093 segments\n",
      "89681 unique passages\n",
      "CharlesII 2625 citations 32753 segments\n",
      "141959 unique passages\n",
      "CivilWar 1743 citations 11986 segments\n",
      "57617 unique passages\n",
      "JamesII 1148 citations 3362 segments\n",
      "13873 unique passages\n",
      "Interregnum 3203 citations 19442 segments\n",
      "96161 unique passages\n"
     ]
    }
   ],
   "source": [
    "for era in os.listdir('../assets/processed'): \n",
    "    if era == \".DS_Store\": continue \n",
    "    process(era,\"chapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get non-citation segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218958"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"../assets\"\n",
    "locations = {}\n",
    "for fp in os.listdir(f\"{folder}/relevant\"):\n",
    "    if not re.search(f'CivilWar',fp): continue\n",
    "    else:\n",
    "      with open(f\"{folder}/relevant/{fp}\",\"r\") as file:\n",
    "        r = json.load(file)\n",
    "        for k, v in r.items(): \n",
    "            loc = v[2]\n",
    "            for l in loc: \n",
    "               locations[tuple(l)] = None \n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:48<00:00,  5.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CivilWar: 16 batches. Total 639551 parts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "for era in os.listdir('../assets/processed'): \n",
    "    if era == \".DS_Store\": continue \n",
    "    if not re.search(\"CivilWar\",era): continue\n",
    "    parts = {}\n",
    "    for prefix in tqdm(os.listdir(f\"../assets/processed/{era}/sub-segments\")):\n",
    "        if prefix == \".DS_Store\": continue\n",
    "\n",
    "        with open(f\"../assets/processed/{era}/sub-segments/{prefix}\",'r') as file:\n",
    "            s_ids, s_text, s_orig, _ = json.load(file)\n",
    "        for idx, s_id in enumerate(s_ids):\n",
    "            tcpID, sidx, nidx = s_id[0][0], str(s_id[0][1]), s_id[0][2]\n",
    "            pidx = s_id[1]\n",
    "            if nidx >= 0: continue # only body content\n",
    "            s = combine_punc_with_text(s_text[idx])\n",
    "            t = combine_punc_with_text(s_orig[idx])\n",
    "            # t = re.sub(r\"\\<\\/i\\>|\\<NOTE\\>|NONLATINALPHABET|\\<i\\>\",\"\",t)\n",
    "            # t = re.sub(r\"\\s+\",\" \",t)\n",
    "            t = t.strip(\" \")\n",
    "            if s not in parts: \n",
    "                parts[s] = ({},{}) # original, locations \n",
    "            if len(s.split(\" \"))< 5: continue # at least 5 words long  \n",
    "            parts[s][1][(tcpID,sidx,str(s_id[1]))] = None \n",
    "            parts[s][0][t] = None \n",
    "    for s,r in parts.items(): \n",
    "        parts[s] = [r[0], list(r[1].keys())]\n",
    "    \n",
    "    batches = []\n",
    "    batch_size = 40000\n",
    "    batch_num = 0 \n",
    "    for i in range(0, len(parts), batch_size):\n",
    "        batch = {}\n",
    "        for p in list(parts.keys())[i: i + batch_size]:\n",
    "            batch[p] = parts[p]\n",
    "        batches.append(batch)\n",
    "        with open(f\"../assets/unique_body/{era}_{batch_num}\",\"w+\") as file: \n",
    "            json.dump(parts,file)\n",
    "        batch_num += 1 \n",
    "    print(f\"{era}: {len(batches)} batches. Total {len(parts)} parts.\")\n",
    "\n",
    "'''\n",
    " CivilWar: 16 batches. Total 639551 parts.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Marginalia \n",
    "from tqdm import tqdm \n",
    "\n",
    "era = \"CivilWar\"\n",
    "parts = {}\n",
    "for fp in tqdm(os.listdir(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}\")):\n",
    "    if \"margin\" not in fp: continue \n",
    "    margin = pd.read_csv(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}/{fp}\", header=None, names=[\"tcpID\",\"sidx\",\"nidx\",\"original\",\"standardized\"])\n",
    "    margin = margin.to_dict(orient=\"records\")\n",
    "    for m in margin:\n",
    "        t = m[\"original\"]\n",
    "        tcpID, sidx, nidx = m[\"tcpID\"], m[\"sidx\"], m[\"nidx\"]\n",
    "        if isinstance(tcpID,int): \n",
    "            print(fp,m)\n",
    "            break\n",
    "        s = m[\"standardized\"]\n",
    "        if not isinstance(s,str): continue \n",
    "        if len(s) == 0: continue \n",
    "        if s not in parts: \n",
    "            parts[s] = ({},{}) # original, locations \n",
    "        parts[s][1][(tcpID,str(sidx),str(nidx))] = None \n",
    "        parts[s][0][t] = None \n",
    "for s,r in parts.items(): \n",
    "    parts[s] = [[str(k) for k in r[0]], list(r[1].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CivilWar: Total 47941 parts.\n"
     ]
    }
   ],
   "source": [
    "with open(f\"../assets/unique_body/{era}_margin\",\"w+\") as file: \n",
    "    json.dump(parts,file)\n",
    "''' \n",
    "CivilWar: Total 47941 parts.\n",
    "'''\n",
    "print(f\"{era}: Total {len(parts)} parts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
