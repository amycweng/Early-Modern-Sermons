{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for information retrieval fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Bible verse to body segments with the relevant citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,json,os\n",
    "import pandas as pd \n",
    "def combine_punc_with_text(segment):\n",
    "    segment = re.sub(r'\\s+([,.?!;:)])', r'\\1', segment)\n",
    "    segment = re.sub(r'([(])\\s+', r'\\1', segment)\n",
    "    segment = re.sub(r\"\\s+\",\" \",segment)\n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(era,c_type=\"verse\"): \n",
    "    relevant = {}\n",
    "    with open(f\"../assets/citations/{era}_{c_type}_citation_segments.json\") as file:\n",
    "        c_to_seg = json.load(file)\n",
    "    seg_to_c = {}\n",
    "    for cited, segments in c_to_seg.items():\n",
    "        if \"Ibidem\" in cited: continue \n",
    "        cited = \" \".join(cited.split(\"-\"))\n",
    "        if re.search(r\"\\d+ \\d+\",cited):\n",
    "            cited = cited.split(\" \")\n",
    "            if c_type == \"verse\": \n",
    "                cited = \" \".cited(cited[:-2]) +\" \" + \".\".join(cited[-2:])\n",
    "            else: cited = \" \".join(cited)\n",
    "        for s in segments: \n",
    "            seg_id = (s.split(\",\")[0],s.split(\",\")[1])\n",
    "            if seg_id not in seg_to_c: \n",
    "                seg_to_c[seg_id] = []\n",
    "            seg_to_c[seg_id].append(cited)\n",
    "    print(era, len(c_to_seg),'citations',len(seg_to_c),\"segments\")\n",
    "\n",
    "    for prefix in os.listdir(f\"../assets/processed/{era}/sub-segments\"):\n",
    "        if prefix == \".DS_Store\": continue\n",
    "        with open(f\"../assets/processed/{era}/sub-segments/{prefix}\",'r') as file:\n",
    "            s_ids, s_text, s_orig, _ = json.load(file)\n",
    "        \n",
    "        for idx, s_id in enumerate(s_ids):\n",
    "            tcpID, sidx, nidx = s_id[0][0], str(s_id[0][1]), s_id[0][2]\n",
    "            if nidx >= 0: continue # only body content\n",
    "            if (tcpID,sidx) in seg_to_c: \n",
    "                s = combine_punc_with_text(s_text[idx])\n",
    "                t = combine_punc_with_text(s_orig[idx])\n",
    "                if s not in relevant: \n",
    "                    relevant[s] = ({},{},{}) # citations, original, location \n",
    "                if len(s.split(\" \"))< 5: continue # at least 5 words long  \n",
    "                relevant[s][2][(tcpID,sidx,str(s_id[1]))] = None \n",
    "                relevant[s][1][t] = None \n",
    "                for c in seg_to_c[(tcpID,sidx)]:\n",
    "                    relevant[s][0][c] = None \n",
    "    for s,r in relevant.items(): \n",
    "        relevant[s] = (list(r[0].keys()), list(r[1].keys()), list(r[2].keys()))\n",
    "    print(len(set(relevant)),'unique passages')\n",
    "    if c_type == \"chapter\": \n",
    "        with open(f\"../assets/relevant/{era}_chapter_citations.json\",\"w+\") as file: \n",
    "            json.dump(relevant, file)\n",
    "    else: \n",
    "        with open(f\"../assets/relevant/{era}.json\",\"w+\") as file: \n",
    "            json.dump(relevant, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elizabethan 8858 verses 8357 segments\n",
      "37274 unique passages\n",
      "Carolinian 18164 verses 25308 segments\n",
      "139871 unique passages\n",
      "WilliamAndMary 12923 verses 18364 segments\n",
      "103595 unique passages\n",
      "pre-Elizabethan 101 verses 77 segments\n",
      "286 unique passages\n",
      "Jacobean 17288 verses 26125 segments\n",
      "133130 unique passages\n",
      "CharlesII 24223 verses 54104 segments\n",
      "318993 unique passages\n",
      "CivilWar 15625 verses 16766 segments\n",
      "105768 unique passages\n",
      "JamesII 5410 verses 3533 segments\n",
      "21208 unique passages\n",
      "Interregnum 18670 verses 29877 segments\n",
      "190009 unique passages\n"
     ]
    }
   ],
   "source": [
    "for era in os.listdir('../assets/processed'): \n",
    "    if era == \".DS_Store\": continue \n",
    "    process(era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elizabethan 1344 citations 5695 segments\n",
      "23698 unique passages\n",
      "Carolinian 1549 citations 7054 segments\n",
      "45759 unique passages\n",
      "WilliamAndMary 1011 citations 2826 segments\n",
      "19027 unique passages\n",
      "pre-Elizabethan 441 citations 823 segments\n",
      "2424 unique passages\n",
      "Jacobean 1583 citations 7679 segments\n",
      "45188 unique passages\n",
      "CharlesII 1819 citations 10002 segments\n",
      "71576 unique passages\n",
      "CivilWar 1265 citations 4174 segments\n",
      "31299 unique passages\n",
      "JamesII 671 citations 874 segments\n",
      "5985 unique passages\n",
      "Interregnum 2681 citations 6366 segments\n",
      "49594 unique passages\n"
     ]
    }
   ],
   "source": [
    "for era in os.listdir('../assets/processed'): \n",
    "    if era == \".DS_Store\": continue \n",
    "    process(era,\"chapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get non-citation segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299273"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"../assets\"\n",
    "locations = {}\n",
    "for fp in os.listdir(f\"{folder}/relevant\"):\n",
    "    if not re.search(f'CivilWar|Interregnum',fp): continue\n",
    "    else:\n",
    "      with open(f\"{folder}/relevant/{fp}\",\"r\") as file:\n",
    "        r = json.load(file)\n",
    "        for k, v in r.items(): \n",
    "            loc = v[2]\n",
    "            for l in loc: \n",
    "               locations[tuple(l)] = None \n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:48<00:00,  5.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CivilWar: 5 batches. Total 628259 parts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:23<00:00,  9.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interregnum: 8 batches. Total 1071663 parts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "for era in os.listdir('../assets/processed'): \n",
    "    if era == \".DS_Store\": continue \n",
    "    if not re.search(\"CivilWar|Interregnum\",era): continue\n",
    "    parts = {}\n",
    "    for prefix in tqdm(os.listdir(f\"../assets/processed/{era}/sub-segments\")):\n",
    "        if prefix == \".DS_Store\": continue\n",
    "\n",
    "        with open(f\"../assets/processed/{era}/sub-segments/{prefix}\",'r') as file:\n",
    "            s_ids, s_text, s_orig, _ = json.load(file)\n",
    "        for idx, s_id in enumerate(s_ids):\n",
    "            tcpID, sidx, nidx = s_id[0][0], str(s_id[0][1]), s_id[0][2]\n",
    "            pidx = s_id[1]\n",
    "            if nidx >= 0: continue # only body content\n",
    "            s = combine_punc_with_text(s_text[idx])\n",
    "            t = combine_punc_with_text(s_orig[idx])\n",
    "            t = re.sub(r\"\\<\\/i\\>|\\<NOTE\\>|NONLATINALPHABET|\\<i\\>\",\"\",t)\n",
    "            t = re.sub(r\"\\s+\",\" \",t)\n",
    "            t = t.strip(\" \")\n",
    "            if s not in parts: \n",
    "                parts[s] = ({},{}) # original, locations \n",
    "            if len(s.split(\" \"))< 5: continue # at least 5 words long  \n",
    "            parts[s][1][(tcpID,sidx,str(s_id[1]))] = None \n",
    "            parts[s][0][t] = None \n",
    "    for s,r in parts.items(): \n",
    "        parts[s] = [r[0], list(r[1].keys())]\n",
    "    \n",
    "    batches = []\n",
    "    batch_size = 150000\n",
    "    batch_num = 0 \n",
    "    for i in range(0, len(parts), batch_size):\n",
    "        batch = {}\n",
    "        for p in list(parts.keys())[i: i + batch_size]:\n",
    "            batch[p] = parts[p]\n",
    "        batches.append(batch)\n",
    "        with open(f\"../assets/unique_body/{era}_{batch_num}\",\"w+\") as file: \n",
    "            json.dump(parts,file)\n",
    "        batch_num += 1 \n",
    "    print(f\"{era}: {len(batches)} batches. Total {len(parts)} parts.\")\n",
    "\n",
    "'''\n",
    "CivilWar: 5 batches. Total 628259 parts.\n",
    "Interregnum: 8 batches. Total 1071663 parts.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CivilWar: 1 batches. Total 47610 parts.\n"
     ]
    }
   ],
   "source": [
    "# Marginalia \n",
    "era = \"CivilWar\"\n",
    "parts = {}\n",
    "for fp in tqdm(os.listdir(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}\")):\n",
    "    if \"margin\" not in fp: continue \n",
    "    margin = pd.read_csv(f\"/Users/amycweng/DH/SERMONS_APP/db/data/{era}/{fp}\", header=None, names=[\"tcpID\",\"sidx\",\"nidx\",\"original\",\"standardized\"])\n",
    "        \n",
    "    for idx, t in enumerate(margin[\"original\"]):\n",
    "        tcpID, sidx, nidx = margin[\"tcpID\"][idx], margin[\"sidx\"][idx], margin[\"nidx\"][idx]\n",
    "        s = margin[\"standardized\"][idx]\n",
    "        if not isinstance(s,str): continue \n",
    "        if len(s) == 0: continue \n",
    "        t = re.sub(r\"\\<\\/i\\>|\\<NOTE\\>|NONLATINALPHABET|\\<i\\>\",\"\",t)\n",
    "        t = re.sub(r\"\\s+\",\" \",t)\n",
    "        t = t.strip(\" \")\n",
    "        if s not in parts: \n",
    "            parts[s] = ({},{}) # original, locations \n",
    "        parts[s][1][(tcpID,str(sidx),str(nidx))] = None \n",
    "        parts[s][0][t] = None \n",
    "for s,r in parts.items(): \n",
    "    parts[s] = [r[0], list(r[1].keys())]\n",
    "\n",
    "batches = []\n",
    "batch_size = 150000\n",
    "batch_num = 0 \n",
    "for i in range(0, len(parts), batch_size):\n",
    "    batch = {}\n",
    "    for p in list(parts.keys())[i: i + batch_size]:\n",
    "        batch[p] = parts[p]\n",
    "    batches.append(batch)\n",
    "    with open(f\"../assets/unique_body/{era}_margin_{batch_num}\",\"w+\") as file: \n",
    "        json.dump(parts,file)\n",
    "    batch_num += 1 \n",
    "''' \n",
    "CivilWar: 1 batches. Total 47610 parts.\n",
    "'''\n",
    "print(f\"{era}: {len(batches)} batches. Total {len(parts)} parts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
