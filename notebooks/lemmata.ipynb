{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "prefixes = ['B','A0','A1','A2','A3','A4','A5','A6','A7','A8','A9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(data,type): \n",
    "    lemmata = {}\n",
    "    tokens = {}\n",
    "    for item in data.values(): \n",
    "        if type != \"margin\": \n",
    "            item = item.items()\n",
    "        for i in item: \n",
    "            encodings = i[1]\n",
    "            for token, pos, lemma in encodings:\n",
    "                token = token.strip(\".\")\n",
    "                pos = pos.strip(\".\")\n",
    "                lemma = lemma.strip(\".\")\n",
    "                # proper nouns, abbreviations, or capitalized words \n",
    "                if token == pos: continue # no punctuation \n",
    "                if len(token) == 0: continue\n",
    "                if token not in tokens: tokens[token] = 0\n",
    "                tokens[token] += 1\n",
    "                if len(pos) == 0: continue \n",
    "                if \"np\" in pos: \n",
    "                    if lemma not in lemmata: lemmata[lemma] = 0\n",
    "                    lemmata[lemma] += 1 \n",
    "                elif token[0].isupper():\n",
    "                    # ALSO LOOK FOR VERBS \n",
    "                    if (\"n\"==pos[0]) or (\"j\" == pos[0]) or ('fw' in pos):  \n",
    "                        if token not in lemmata: lemmata[token] = 0\n",
    "                        lemmata[token] += 1 \n",
    "    return lemmata, tokens\n",
    "\n",
    "def add_to_dict(old,new): \n",
    "    for word, freq in new.items(): \n",
    "        if word not in old: old[word] = freq\n",
    "        else: old[word] += freq\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "A0\n",
      "A1\n",
      "A2\n",
      "A3\n",
      "A4\n",
      "A5\n",
      "A6\n",
      "A7\n",
      "A8\n",
      "A9\n"
     ]
    }
   ],
   "source": [
    "lemmata = {}\n",
    "tokens = {}\n",
    "for prefix in prefixes: \n",
    "    with open(f\"../assets/processed/{prefix}_marginalia.json\",\"r\") as file: \n",
    "        data = json.load(file)\n",
    "    l, t = get_words(data,\"margin\")\n",
    "    lemmata = add_to_dict(lemmata,l)\n",
    "    tokens = add_to_dict(tokens, t)\n",
    "    print(prefix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170404 types\n",
      "2464705 tokens\n",
      "54624 possible proper nouns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Psal', 24247),\n",
       " ('Cor', 17266),\n",
       " ('Rom', 13450),\n",
       " ('Mat', 11644),\n",
       " ('Gen', 10244),\n",
       " ('John', 8129),\n",
       " ('Heb', 7853),\n",
       " ('Sam', 7533),\n",
       " ('Pet', 6718),\n",
       " ('Tim', 6571),\n",
       " ('God', 6535),\n",
       " ('Act', 5830),\n",
       " ('Isa', 5208),\n",
       " ('Luk', 5143),\n",
       " ('Matth', 5021),\n",
       " ('Exod', 4768),\n",
       " ('Aug', 4659),\n",
       " ('Christ', 4652),\n",
       " ('Ioh', 4615),\n",
       " ('Job', 4315)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokens), 'types')\n",
    "print(sum(tokens.values()), 'tokens')\n",
    "print(len(lemmata),'possible proper nouns')\n",
    "lemmata = sorted(lemmata.items(), key=lambda x:x[1], reverse=True)\n",
    "tokens = sorted(tokens.items(), key=lambda x:x[1], reverse=True)\n",
    "lemmata[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/vocab/marginal_word_types.json\",\"w+\") as file: \n",
    "    json.dump(tokens, file)\n",
    "with open(\"../assets/vocab/marginal_proper_nouns.json\",\"w+\") as file: \n",
    "    json.dump(lemmata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lemmata = {}\n",
    "text_tokens = {}\n",
    "def get_group_vocab(prefix,text_tokens,text_lemmata): \n",
    "    with open(f\"../assets/processed/{prefix}_texts.json\",\"r\") as file: \n",
    "        data = json.load(file)\n",
    "    l, t = get_words(data,\"text\")\n",
    "    text_lemmata = add_to_dict(text_lemmata,l)\n",
    "    text_tokens = add_to_dict(text_tokens, t)\n",
    "    print(prefix)\n",
    "    return text_tokens,text_lemmata\n",
    "\n",
    "def save(text_tokens, text_lemmata):\n",
    "    with open(\"../assets/vocab/text_word_types.json\",\"w+\") as file: \n",
    "        json.dump(text_tokens, file)\n",
    "    with open(\"../assets/vocab/text_proper_nouns.json\",\"w+\") as file: \n",
    "        json.dump(text_lemmata, file)\n",
    "    print('saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "saved!\n",
      "A0\n",
      "saved!\n",
      "A1\n",
      "saved!\n",
      "A2\n",
      "saved!\n",
      "A3\n",
      "saved!\n",
      "A4\n",
      "saved!\n",
      "A5\n",
      "saved!\n",
      "A6\n",
      "saved!\n",
      "A7\n",
      "saved!\n",
      "A8\n",
      "saved!\n",
      "A9\n",
      "saved!\n"
     ]
    }
   ],
   "source": [
    "for prefix in prefixes: \n",
    "    text_tokens, text_lemmata = get_group_vocab(prefix,text_tokens, text_lemmata)\n",
    "    save(text_tokens, text_lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597004 types\n",
      "109755841 tokens\n",
      "165367 possible proper nouns\n",
      "saved!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('God', 1031383),\n",
       " ('Christ', 404056),\n",
       " ('Lord', 230838),\n",
       " ('Church', 102753),\n",
       " ('Jesus', 81429),\n",
       " ('Spirit', 69577),\n",
       " ('Paul', 58750),\n",
       " ('World', 56951),\n",
       " ('Father', 56329),\n",
       " ('David', 56099),\n",
       " ('Apostle', 56000),\n",
       " ('King', 54175),\n",
       " ('Law', 53038),\n",
       " ('Faith', 52938),\n",
       " ('Heaven', 52614),\n",
       " ('Religion', 52358),\n",
       " ('Gods', 49195),\n",
       " ('Man', 47392),\n",
       " ('Men', 46011),\n",
       " ('Christian', 45637)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text_tokens), 'types')\n",
    "print(sum(text_tokens.values()), 'tokens')\n",
    "print(len(text_lemmata),'possible proper nouns')\n",
    "text_lemmata = sorted(text_lemmata.items(), key=lambda x:x[1], reverse=True)\n",
    "text_tokens = sorted(text_tokens.items(), key=lambda x:x[1], reverse=True)\n",
    "save(text_tokens, text_lemmata)\n",
    "text_lemmata[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/vocab/marginal_word_types.json\",\"r\") as file: \n",
    "    tokens = json.load(file)\n",
    "with open(\"../assets/vocab/marginal_proper_nouns.json\",\"r\") as file: \n",
    "    lemmata = json.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
