{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, re, shutil,json\n",
    "import pandas as pd \n",
    "EP = '/Users/amycweng/Digital Humanities/eebotcp/texts'\n",
    "TCP = '/Users/amycweng/Digital Humanities/TCP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify texts that contain sermons in EEBO-TCP using metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataFolder = '/Users/amycweng/Digital Humanities/ECBC-Data-2022/TCP metadata'\n",
    "sermons = []\n",
    "count_subjectheadings = []\n",
    "hit = False\n",
    "for csvFile in os.listdir(metadataFolder):\n",
    "    data = pd.read_csv(os.path.join(metadataFolder,csvFile))\n",
    "    for idx,tcpID in enumerate(data['id']):\n",
    "        hit = False\n",
    "        # columns of the TCP metadata CSVs: \n",
    "        # id,stc,estc,title,author,publisher,pubplace,subject_headings,date\n",
    "        orig_title = data['title'][idx]\n",
    "        title = orig_title.lower().replace(\"'\",'')\n",
    "        estc = data['estc'][idx]\n",
    "        stc= data['stc'][idx]\n",
    "        authors = data['author'][idx].split(\"; \")\n",
    "        authors = \"; \".join(set(authors))\n",
    "        publisher = data['publisher'][idx]\n",
    "        pubplace = data['pubplace'][idx]\n",
    "        date = f\"{data['date'][idx]}\"\n",
    "\n",
    "\n",
    "        subject_headings = data['keywords'][idx].replace(\" -- \",\"; \").replace(\"  \",\" \")\n",
    "        # search in the subject headings \n",
    "        if re.search('sermon', subject_headings.lower()): \n",
    "            hit = True \n",
    "            count_subjectheadings.append(tcpID) \n",
    "        # search in title \n",
    "        elif re.search(r'preached|preacht', title): \n",
    "            hit = True\n",
    "\n",
    "        if hit: \n",
    "            sermons.append( {\"id\": tcpID, \n",
    "                              \"estc\":estc,\n",
    "                              \"stc\":stc,\n",
    "                              \"title\": orig_title,\n",
    "                              \"authors\": authors,\n",
    "                              \"publisher\": publisher,\n",
    "                              \"pubplace\":pubplace,\n",
    "                              \"subject_headings\":subject_headings,\n",
    "                              \"date\":date\n",
    "                              }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5535 TCP XML files are likely to contain sermons.\n",
      "5241 texts containing sermons are identified by the Library of Congress' subject headings\n",
      "294 texts are not identified by subject heading as a sermon but mention 'preached' or 'preacht' in the title\n"
     ]
    }
   ],
   "source": [
    "# store relevant metadata in a CSV file \n",
    "with open(\"sermons.csv\",\"w+\") as outfile: \n",
    "    writer = csv.DictWriter(outfile, fieldnames=sermons[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sermons)\n",
    "\n",
    "with open(\"sermons_subjectheadings.json\",\"w+\") as outfile: \n",
    "    json.dump(count_subjectheadings,outfile)\n",
    "\n",
    "print(f\"{len(sermons)} TCP XML files are likely to contain sermons.\") \n",
    "print(f\"{len(count_subjectheadings)} texts containing sermons are identified by the Library of Congress' subject headings\")\n",
    "print(f\"{len(sermons)-len(count_subjectheadings)} texts are not identified by subject heading as a sermon but mention 'preached' or 'preacht' in the title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy all the TCP XML files of the texts to a separate folder for easy browsing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sermons = pd.read_csv(\"sermons.csv\")[\"id\"]\n",
    "\n",
    "def findTextTCP(id):\n",
    "    if re.match('B1|B4',id[0:2]):\n",
    "        path = f'{TCP}/P2{id[0:2]}/{id}.P4.xml'\n",
    "    else: \n",
    "        if f'{id}.P4.xml' in os.listdir(f'{TCP}/P1{id[0:2]}'):\n",
    "            path = f'{TCP}/P1{id[0:2]}/{id}.P4.xml'\n",
    "        elif f'{id}.P4.xml' in os.listdir(f'{TCP}/P2{id[0:2]}'): \n",
    "            path = f'{TCP}/P2{id[0:2]}/{id}.P4.xml'\n",
    "    return path \n",
    "\n",
    "for s in sermons: \n",
    "    path = findTextTCP(s)\n",
    "    shutil.copy(path,'/Users/amycweng/Digital Humanities/sermonsTCP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy all the EP (EarlyPrint) XML files of the texts to a separate folder for easy browsing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "underscores = {}\n",
    "def findTextEP(tcpID):\n",
    "    path = None\n",
    "    if \"B43\" in s: return None # there is no folder in the EP texts that starts with B43 \n",
    "    \n",
    "    for file in os.listdir(f'{EP}/{tcpID[0:3]}'):\n",
    "        if tcpID in file: \n",
    "            if '_' in file: \n",
    "                print(file)\n",
    "                # account for the fact that some individual TCP files have been \n",
    "                # sectioned into multiple EP files due to size \n",
    "                if tcpID not in underscores: \n",
    "                    underscores[tcpID] = [f'{EP}/{tcpID[0:3]}/{file}']\n",
    "                else: \n",
    "                    underscores[tcpID].append(f'{EP}/{tcpID[0:3]}/{file}')\n",
    "            else: \n",
    "                path = f'{EP}/{tcpID[0:3]}/{file}' \n",
    "                break   \n",
    "    return path\n",
    "\n",
    "missing = []\n",
    "for s in sermons: \n",
    "    path = findTextEP(s)\n",
    "    if not path: \n",
    "        missing.append(s)\n",
    "    else: \n",
    "        shutil.copy(path,'/Users/amycweng/Digital Humanities/sermonsEP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "There are 5108 sermons in EP. 427 TCP sermons are missing from EP.\n",
      "The TCP ids of the missing texts:  ['A95720', 'B25417', 'B26318', 'B29175', 'B25291', 'B23227', 'B26965', 'B29289', 'B26662', 'B29151', 'B23303', 'B29077', 'B23225', 'B25935', 'B24387', 'B26367', 'B24299', 'B27584', 'B29537', 'B22572', 'B24122', 'B28835', 'B22620', 'B26664', 'B22604', 'B21648', 'B21644', 'B25240', 'B23299', 'B23761', 'B28285', 'B20731', 'B26787', 'B27727', 'B23952', 'B20800', 'B26345', 'B26249', 'B23004', 'B21561', 'B23636', 'B26742', 'B21646', 'B27417', 'B27684', 'B26659', 'B22979', 'B28836', 'B29538', 'B23961', 'B23001', 'B23750', 'B28382', 'B29195', 'B23013', 'B20167', 'B28834', 'B26321', 'B26180', 'B26466', 'B23007', 'B27952', 'B21317', 'B26784', 'B27215', 'B26622', 'B26677', 'B22621', 'B26714', 'B24300', 'B22963', 'B20883', 'B21647', 'B22971', 'B26839', 'B28085', 'B25756', 'B24496', 'B20173', 'B31833', 'B14422', 'B14334', 'B15398', 'B14338', 'B13906', 'B13601', 'B15555', 'B15275', 'B14200', 'B14420', 'B13693', 'B11014', 'B15202', 'B16321', 'B15678', 'B15392', 'B12105', 'B17542', 'B15290', 'B17696', 'B18355', 'B14434', 'B15498', 'B17254', 'B15443', 'B10006', 'B18399', 'B15274', 'B13612', 'B11965', 'B14203', 'B15663', 'B14201', 'B14433', 'B17322', 'B18511', 'B11412', 'B14618', 'B13878', 'B14862', 'B12016', 'B13696', 'B11963', 'B14274', 'B17422', 'B17273', 'B14673', 'B14661', 'B16379', 'B11273', 'B15499', 'B13694', 'B17324', 'B18442', 'B11957', 'B15049', 'B04329', 'B09126', 'B00238', 'B09870', 'B09423', 'B02801', 'B06496', 'B07676', 'B04401', 'B01212', 'B09520', 'B03547', 'B02739', 'B09510', 'B03902', 'B09463', 'B04879', 'B09386', 'B43766', 'B43800', 'B43856', 'B43574', 'A85138', 'A85706', 'A82833', 'A88962', 'A84757', 'A87496', 'A81890', 'A85235', 'A81103', 'A87349', 'A87518', 'A84378', 'A88931', 'A80849', 'A81243', 'A86705', 'A85657', 'A88692', 'A87559', 'A84147', 'A88921', 'A81783', 'A85305', 'A85792', 'A84071', 'A85236', 'A81203', 'A82316', 'A84922', 'A84178', 'A85776', 'A80425', 'A83565', 'A81102', 'A86645', 'A88934', 'A87667', 'A81246', 'A81645', 'A81299', 'A85204', 'A96437', 'A96315', 'A96340', 'A96868', 'A96593', 'A96327', 'A92420', 'A92859', 'A91907', 'A90409', 'A91745', 'A95629', 'A93904', 'A91479', 'A91543', 'A93229', 'A91847', 'A90294', 'A96318', 'A95792', 'A96338', 'A92944', 'A78223', 'A76408', 'A74835', 'A76226', 'A70212', 'A77998', 'A76218', 'A76502', 'A72114', 'A72065', 'A77992', 'A73904', 'A76362', 'A73176', 'A72056', 'A76223', 'A70915', 'A76205', 'A66929', 'A65934', 'A65420', 'A65815', 'A66475', 'A62564', 'A68244', 'A62289', 'A60951', 'A65925', 'A67475', 'A48191', 'A48349', 'A44781', 'A48406', 'A47082', 'A42150', 'A44563', 'A47337', 'A40987', 'A46728', 'A40429', 'A48486', 'A40106', 'A44565', 'A40433', 'A40784', 'A44577', 'A44441', 'A47342', 'A49518', 'A41460', 'A49504', 'A40431', 'A41153', 'A46706', 'A41141', 'A40434', 'A42120', 'A46727', 'A41324', 'A40109', 'A46822', 'A41902', 'A41502', 'A40363', 'A46826', 'A47524', 'A49157', 'A40430', 'A41635', 'A45240', 'A48968', 'A49405', 'A56754', 'A50208', 'A53568', 'A52014', 'A50232', 'A50888', 'A50422', 'A50160', 'A51087', 'A55842', 'A52002', 'A50228', 'A50224', 'A50193', 'A52739', 'A56907', 'A50233', 'A50114', 'A51356', 'A58945', 'A59547', 'A53928', 'A50132', 'A54894', 'A50223', 'A58892', 'A57612', 'A56925', 'A50227', 'A52027', 'A58375', 'A50213', 'A55394', 'A55875', 'A50192', 'A57065', 'A59273', 'A14760', 'A14832', 'A19715', 'A14766', 'A19995', 'A13550', 'A15835', 'A19722', 'A00822', 'A04221', 'A09759', 'A02395', 'A03927', 'A05192', 'A01906', 'A05198', 'A03309', 'A04367', 'A09748', 'A06749', 'A01431', 'A04573', 'A00684', 'A02723', 'A06872', 'A04575', 'A00944', 'A28266', 'A28221', 'A29335', 'A20588', 'A20150', 'A20670', 'A20528', 'A29098', 'A29334', 'A21655', 'A20411', 'A29332', 'A21258', 'A21260', 'A21055', 'A28336', 'A32938', 'A35322', 'A39579', 'A35318', 'A36517', 'A35691', 'A39699', 'A36327', 'A35176', 'A37266', 'A35324', 'A38685', 'A39746', 'A39284', 'A39115', 'A39851', 'A37203', 'A39904', 'A32214', 'A39788', 'A35323', 'A39991', 'A37265', 'A36855', 'A39743', 'A36733', 'A37204', 'A36514', 'A36234', 'A36880', 'A35173', 'A35753', 'A39968', 'A36853', 'A38495', 'A31406', 'A36455', 'A39399', 'A36772', 'A34196', 'A37261', 'A67819', 'A76482', 'A72359', 'A45574', 'A00164', 'A00156', 'A00687', 'A06325', 'A04924', 'A19744', 'A17050', 'A19485', 'A21251']\n"
     ]
    }
   ],
   "source": [
    "print(underscores)\n",
    "print(f\"There are {len(sermons)-len(missing)} sermons in EP. {len(missing)} TCP sermons are missing from EP.\")\n",
    "print(\"The TCP ids of the missing texts: \", missing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
